{
  "hash": "4a19f8c7a917698ed1a7c110b92b6e27",
  "result": {
    "markdown": "---\ntitle: \"Hate Speech Klassifikation\"\nauthor: \"Nils Biller\"\ndate: \"2024-02-09\"\ncategories: [Textanalyse, Tidymodels, Klassifikation, Transformers, Neuronale Netze]\nimage: HateTastatur.jpeg\n---\n\n\n# Hate Speech Klassifikation\n\nDie Klassifikation von Hassrede in Textnachrichten ist entscheidend, um die Auswirkungen von negativem Verhalten in sozialen Medien zu verstehen und einzudämmen. Durch die Entwicklung von Algorithmen und Modellen, die automatisch Hassrede erkennen, können wir potenziell schädliche Inhalte identifizieren und bekämpfen. Dieser Post bietet einen Überblick über einige Ansätze und Techniken zur Erkennung von Hassrede.\n\nZur Demonstration der Modelle wird ein Datensatz mit Twitternachrichten verwendet.\n\n## 1 Vorbereitung der Daten\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\nlibrary(tidytext)\nlibrary(tidymodels)\nlibrary(wordcloud)\nlibrary(tictoc)\nlibrary(tm) #für Corpus\nlibrary(caret) #für Konfusionsmatrix\nlibrary(textrecipes) #für u.a. step_textfeature\nlibrary(syuzhet) #Stimmungsanalyse\nlibrary(readxl) #für Exeldateien\nlibrary(xgboost)\n```\n:::\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nd_hate <- read_csv(pfad)\n```\n:::\n\n\n### 1.1 Datensatz bereinigen\n\n\n::: {.cell}\n\n```{.r .cell-code}\n## Tweets bereinigen\nclean.text = function(x)\n{\n  x = tolower(x)                 # alles in Kleinbuchstaben umwandeln\n  x = gsub(\"rt\", \"\", x)          # rt entfernen\n  x = gsub(\"@\\\\w+\", \"\", x)       # alle @ + Namen entfernen\n  x = gsub(\"[[:punct:]]\", \"\", x) # Satzzeichen entfernen\n  x = gsub(\"[[:digit:]]\", \"\", x) # Zahlen entfernen\n  x = gsub(\"http\\\\w+\", \"\", x)    # Links entfernen\n  x = gsub(\"http\", \"\", x)        # hhtp entfernen wenn es alleine steht\n  x = gsub(\"[ |\\t]{2,}\", \" \", x) # doppelte Leerzeichen und Tabs entfernen\n  x = gsub(\"^ \", \"\", x)          # Leerzeichen am Anfang entfernen\n  x = gsub(\" $\", \"\", x)          # Leerzeichen am Ende entfernen\n\n  return(x)\n}\n\n#Funktion anwenden\ndf_hate <- d_hate %>% \n  mutate(tweet_clean = tweet) %>% \n  mutate(tweet_clean = clean.text(tweet_clean))\n\n#Splitten\nset.seed(142)\ntrain_test_split <- initial_split(df_hate, strata = class)\ntrain_hate <- training(train_test_split)\ntest_hate <- testing(train_test_split)\n```\n:::\n\n\n### 1.2 Stoppwörter entfernen\n\nNachdem die Tweets nun bereinigt sind, können die Stoppwörter entfernt werden, da sie in der Regel keine besondere Rolle bei der Interpretation eines Satzes spielen.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# unnest\ndf_hate <- df_hate %>%\n  mutate(words = tweet_clean) %>% \n  unnest_tokens(words, words)\n\n## die 15 häufigsten Wörter\ndf_hate %>%\n  count(words, sort = TRUE) %>%\n  top_n(15) %>%\n  mutate(words = reorder(words, n)) %>%\n  ggplot(aes(x = words, y = n)) +\n  geom_col() +\n  xlab(NULL) +\n  coord_flip() +\n      labs(x = \"Wort\",\n      y = \"Anzahl\",\n      title = \"Die 15 häufigsten Wörter\") +\n  theme_minimal()\n```\n\n::: {.cell-output-display}\n![](Abgabe_files/figure-html/unnamed-chunk-3-1.png){width=672}\n:::\n:::\n\n\nDie neun häufigsten Wörter sind Stoppwörter. Dies demonstriert, dass es sinnvoll ist diese zu entfernen.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Stoppwortentfernung\ndata(\"stop_words\")\n\ndf_hate <- df_hate %>%\n  anti_join(stop_words, by = c(\"words\" = \"word\"))\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n## die 15 häufigsten Wörter\ndf_hate %>%\n  count(words, sort = TRUE) %>%\n  top_n(15) %>%\n  mutate(words = reorder(words, n)) %>%\n  ggplot(aes(x = words, y = n)) +\n  geom_col() +\n  xlab(NULL) +\n  coord_flip() +\n      labs(x = \"Wort\",\n      y = \"Anzahl\",\n      title = \"Die 15 häufigsten Wörter\") +\n  theme_minimal()\n```\n\n::: {.cell-output-display}\n![](Abgabe_files/figure-html/unnamed-chunk-4-1.png){width=672}\n:::\n:::\n\n\nDas Entfernen der Stoppwörter war erfolgreich. Das nun mit Abstand häufigste Wort ist \"trash\".\n\n## 2 Visualisierung der Tweets\n\nUm einen Überblick über die Tweets zu erhalten, werden diese im Folgenden grafisch analysiert.\n\n### 2.1 Stimmungsanalyse\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Converting tweets to ASCII, um Verarbeitung zu garantieren\ntweets <- df_hate %>% \n  select(tweet_clean)\ntweets <- iconv(tweets, from=\"UTF-8\", to=\"ASCII\", sub=\"\")\n\n#Sentiments erkennen\nsentiments <- get_nrc_sentiment((tweets))\n\n#Score erstellen\nsentimentscores<-data.frame(colSums(sentiments[,]))\n\n#Spalte umbenennen\nnames(sentimentscores) <- \"Score\"\n\n#neue Spalte \"sentiment\"\nsentimentscores <- cbind(\"sentiment\"=rownames(sentimentscores),sentimentscores)\nrownames(sentimentscores) <- NULL\n\n#plotten\nggplot(data=sentimentscores,aes(x=sentiment,y=Score))+\n  geom_bar(aes(fill=sentiment),stat = \"identity\")+\n  theme(legend.position=\"none\")+\n  xlab(\"Sentiments\")+ylab(\"Scores\")+\n  ggtitle(\"Stimmung in den Tweets\")+\n  theme_minimal()+\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n```\n\n::: {.cell-output-display}\n![](Abgabe_files/figure-html/unnamed-chunk-5-1.png){width=672}\n:::\n:::\n\n\nDiese Graphik bietet einen schönen Überblick über die Stimmung in den Tweets. Positive und Negative Tweets sind relativ ausgeglichen. Jede Stimmung ist ähnlich oft vorhanden.\n\n### 2.2 Wordcloud\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Worttoken umwandeln\ntweets <- Corpus(VectorSource(df_hate$words))\n\nset.seed(42)\nwordcloud(tweets, min.freq = 1, max.words = 100, scale = c(2.2,1),\n          colors=brewer.pal(8, \"Dark2\"), random.color = T, random.order = F,\n          rot.per=0.25)\n```\n\n::: {.cell-output-display}\n![](Abgabe_files/figure-html/unnamed-chunk-6-1.png){width=672}\n:::\n:::\n\n\nHier sieht man die häufigsten Wörter aus den Tweets.\n\nUnterscheidet man zwischen Tweets die als Hate Speech kassifiziert sind und denen die kein Hate Speech sind, bekommt man folgende Wordclouds.\n\n::: panel-tabset\n## Hate Speech\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\" code-summary=\"Show the Code\"}\n## Wortwolke Hate Speech\n\n# Worttoken umwandeln\ntweets <- df_hate %>% \n  filter(class == \"hate speech\")\n\ntweets <- Corpus(VectorSource(tweets$words))\n\n#Cloud erstellen\nset.seed(42)\nwordcloud(tweets, min.freq = 1, max.words = 100, scale = c(2.2,1),\n          colors=brewer.pal(8, \"Dark2\"), random.color = T, random.order = F,\n          rot.per=0.25)\n```\n\n::: {.cell-output-display}\n![](Abgabe_files/figure-html/unnamed-chunk-7-1.png){width=672}\n:::\n:::\n\n\n## No Hate Speech\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\" code-summary=\"Show the Code\"}\n## Wortwolke Non Hate Speech\n\n# Worttoken umwandeln\ntweets <- df_hate %>% \n  filter(class != \"hate speech\")\n\ntweets <- Corpus(VectorSource(tweets$words))\n\n#Cloud erstellen\nset.seed(42)\nwordcloud(tweets, min.freq = 1, max.words = 100, scale = c(2.2,1),\n          colors=brewer.pal(8, \"Dark2\"), random.color = T, random.order = F,\n          rot.per=0.25)\n```\n\n::: {.cell-output-display}\n![](Abgabe_files/figure-html/unnamed-chunk-8-1.png){width=672}\n:::\n:::\n\n:::\n\nAuffällig ist, dass bei No Hate Speech auch das Wort \"hoe\" vorkommt (zu sehen rechts unten Mitte).\n\nErklärt werden kann dies mit folgendem Tweet.\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\" code-summary=\"Show the Code\"}\ntweets <- df_hate %>% \n  filter(class != \"hate speech\") %>% \n  filter(words == \"hoe\")\n\ntweets[5, \"tweet_clean\"]\n```\n\n::: {.cell-output-display}\n`````{=html}\n<div data-pagedtable=\"false\">\n  <script data-pagedtable-source type=\"application/json\">\n{\"columns\":[{\"label\":[\"tweet_clean\"],\"name\":[1],\"type\":[\"chr\"],\"align\":[\"left\"]}],\"data\":[{\"1\":\"hoe hoe hoe merry christmas\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\n  </script>\n</div>\n`````\n:::\n:::\n\n\nHomonyme wie dieses, könnten zu Fehlern in der Vorhersage führen.\n\n### 2.3 Wortbeziehungen (Bigram)\n\nDie am häufigsten vorkommenden Wortpaare sind in folgender interaktiven Graphik zu sehen.\n\n*Navigation:* rechte Maustaste: bewegen; Mausrad: zoomen\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\" code-summary=\"Show the Code\"}\n## Bigram interaktiv\n#bigram Tabelle\nbigram <- df_hate %>% \n  unnest_tokens(\n    input = tweet_clean, \n    output = bigram, \n    token = 'ngrams', \n    n = 2\n  ) %>% \n  filter(! is.na(bigram))\n\nbigram %<>% \n  separate(col = bigram, into = c('word1', 'word2'), sep = ' ') %>% \n  filter(! word1 %in% stop_words$word) %>% \n  filter(! word2 %in% stop_words$word) %>% \n  filter(! is.na(word1)) %>% \n  filter(! is.na(word2)) \n\nbigram_count <- bigram %>% \n  count(word1, word2, sort = TRUE) %>% \n  rename(weight = n)\n\n\n# bigram erstellen\nlibrary(networkD3)\nlibrary(igraph)\nlibrary(magrittr)\n\nthreshold <- 50\n\nnetwork <-  bigram_count %>%\n  filter(weight > threshold) %>%\n  graph_from_data_frame(directed = FALSE)\n\n# Anzahl der Kanten\nV(network)$degree <- strength(graph = network)\n# Gewichtung\nE(network)$width <- E(network)$weight/max(E(network)$weight)\n\n# Create networkD3 object.\nnetwork.D3 <- igraph_to_networkD3(g = network)\n# Define node size.\nnetwork.D3$nodes %<>% mutate(Degree = (1E-2)*V(network)$degree)\n# Define color group\nnetwork.D3$nodes %<>% mutate(Group = 1)\n# Define edges width. \nnetwork.D3$links$Width <- 10*E(network)$width\n\nforceNetwork(\n  Links = network.D3$links, \n  Nodes = network.D3$nodes, \n  Source = 'source', \n  Target = 'target',\n  NodeID = 'name',\n  Group = 'Group', \n  opacity = 0.9,\n  Value = 'Width',\n  Nodesize = 'Degree', \n  # We input a JavaScript function.\n  linkWidth = JS(\"function(d) { return Math.sqrt(d.value); }\"), \n  fontSize = 12,\n  zoom = TRUE, \n  opacityNoHover = 1\n)\n```\n\n::: {.cell-output-display}\n```{=html}\n<div class=\"forceNetwork html-widget html-fill-item-overflow-hidden html-fill-item\" id=\"htmlwidget-d23117f5a52d78b2d509\" style=\"width:100%;height:464px;\"></div>\n<script type=\"application/json\" data-for=\"htmlwidget-d23117f5a52d78b2d509\">{\"x\":{\"links\":{\"source\":[5,1,35,23,4,2,0,2,37,1,5,21,1,0,16,2,15,25,28,16,18,25,3,24,7,29,2,8,19,2,33,0,20,26,30,1,38,14,32,1,33,12,9,11,10,15,38,1,5,5,26,6,10,12,0,25],\"target\":[67,41,66,53,28,8,8,8,37,43,57,52,39,17,17,18,63,64,60,50,27,55,40,54,45,61,19,19,19,59,34,51,51,58,62,56,68,48,65,42,35,22,12,47,46,49,71,70,31,31,69,44,13,13,5,36],\"value\":[10,6.27906976744186,4.16490486257928,4.12262156448203,4.03805496828753,3.42494714587738,3.42494714587738,3.29809725158562,3.23467230443975,3.15010570824524,2.91754756871036,2.91754756871036,2.91754756871036,2.91754756871036,2.91754756871036,2.91754756871036,2.91754756871036,2.83298097251586,2.72727272727273,2.45243128964059,2.45243128964059,2.15644820295983,2.00845665961945,1.90274841437632,1.8816067653277,1.81818181818182,1.71247357293869,1.52219873150106,1.52219873150106,1.52219873150106,1.50105708245243,1.47991543340381,1.45877378435518,1.41649048625793,1.3953488372093,1.35306553911205,1.35306553911205,1.35306553911205,1.33192389006342,1.33192389006342,1.28964059196617,1.28964059196617,1.26849894291755,1.24735729386892,1.20507399577167,1.18393234672304,1.18393234672304,1.18393234672304,1.16279069767442,1.16279069767442,1.14164904862579,1.14164904862579,1.12050739957717,1.09936575052854,1.09936575052854,1.09936575052854],\"colour\":[\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\"]},\"nodes\":{\"name\":[\"white\",\"charlie\",\"ass\",\"derek\",\"flappy\",\"trash\",\"uncle\",\"asian\",\"bitch\",\"park\",\"rated\",\"red\",\"slope\",\"top\",\"rick\",\"yankee\",\"colored\",\"da\",\"faggot\",\"nigga\",\"black\",\"ice\",\"slippery\",\"happy\",\"international\",\"yankees\",\"tcot\",\"fucking\",\"bird\",\"social\",\"sole\",\"talk\",\"world\",\"disrupts\",\"operation\",\"shylock\",\"york\",\"blah\",\"yellow\",\"crist\",\"jeter\",\"baker\",\"sheen\",\"brown\",\"tom\",\"massage\",\"spa\",\"sox\",\"scott\",\"stadium\",\"folk\",\"people\",\"cream\",\"bihday\",\"law\",\"game\",\"rangel\",\"cans\",\"pjnet\",\"niggas\",\"flu\",\"media\",\"purpose\",\"fan\",\"fans\",\"series\",\"banking\",\"bag\",\"rice\",\"teapay\",\"strong\",\"starburst\"],\"group\":[1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1],\"nodesize\":[7.07,9,5.48,1.95,1.91,8.12,1.49,1.38,3.06,1.38,2.76,1.38,3.57,2.76,1.34,1.92,2.18,1.63,1.62,4.36,0.89,0.86,0.81,0.72,0.72,1.9,1.22,0.67,2.55,0.64,0.64,2.14,0.6,1.12,0.56,1.12,0.55,1.08,1.06,2.97,1.95,1.62,1.62,1.56,1.49,1.38,1.38,1.38,1.34,1.29,1.16,2.05,0.86,0.72,0.72,0.72,0.71,0.7,0.69,0.66,0.64,0.64,0.64,0.63,0.63,0.6,0.56,0.55,0.54,0.53,0.52,0.52]},\"options\":{\"NodeID\":\"name\",\"Group\":\"Group\",\"colourScale\":\"d3.scaleOrdinal(d3.schemeCategory20);\",\"fontSize\":12,\"fontFamily\":\"serif\",\"clickTextSize\":30,\"linkDistance\":50,\"linkWidth\":\"function(d) { return Math.sqrt(d.value); }\",\"charge\":-30,\"opacity\":0.9,\"zoom\":true,\"legend\":false,\"arrows\":false,\"nodesize\":true,\"radiusCalculation\":\" Math.sqrt(d.nodesize)+6\",\"bounded\":false,\"opacityNoHover\":1,\"clickAction\":null}},\"evals\":[],\"jsHooks\":[]}</script>\n```\n:::\n:::\n\n\n## 3 Huggingface Modell\n\nUm Hate Speech in Textnachrichten zu klassifizieren, können vortrainierte Modelle wie das \"roberta\" Modell von Hugging Face verwendet werden. Dieses Modell wurde auf Grundlage von 40.000 Textnachrichten entwickelt.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(reticulate)\nuse_virtualenv(\"~/Studium/blognilsbiller/VirtualEnv\")\n```\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\nfrom transformers import pipeline\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nWARNING:tensorflow:From C:\\Users\\NILS~1.DES\\DOCUME~1\\Studium\\BLOGNI~1\\VIRTUA~1\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n```\n:::\n\n```{.python .cell-code}\nimport tensorflow\n```\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\nclassifier = pipeline(\"text-classification\", model=\"facebook/roberta-hate-speech-dynabench-r4-target\")\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ntweets <- test_hate$tweet_clean\n```\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\ntweets = r.tweets\nresults = classifier(tweets)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nresult <- py$results\n\n# Extrahieren der Labels\nlabels <- lapply(result, function(element) element$label)\n\n# Zusammenführen der Ergebnisse mit dem ursprünglichen Datensatz\ntweets_zsm <- bind_cols(test_hate, pred = unlist(labels))\n\n# Umwandeln in Faktoren und Umbenennen der Vorhersagen\ntweets_zsm <- tweets_zsm %>%\n  mutate(pred = factor(case_when(pred == \"hate\" ~ \"hate speech\",\n                                 pred == \"nothate\" ~ \"other\")))\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n#Vorhersage\nmy_metrics <- metric_set(accuracy, f_meas)\nmy_metrics(tweets_zsm,\n           truth = as.factor(class), #estimate ist factor, also class anpassen\n           estimate = pred)\n```\n\n::: {.cell-output-display}\n`````{=html}\n<div data-pagedtable=\"false\">\n  <script data-pagedtable-source type=\"application/json\">\n{\"columns\":[{\"label\":[\".metric\"],\"name\":[1],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\".estimator\"],\"name\":[2],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\".estimate\"],\"name\":[3],\"type\":[\"dbl\"],\"align\":[\"right\"]}],\"data\":[{\"1\":\"accuracy\",\"2\":\"binary\",\"3\":\"0.8784846\"},{\"1\":\"f_meas\",\"2\":\"binary\",\"3\":\"0.7976190\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\n  </script>\n</div>\n`````\n:::\n:::\n\n\nDie Metrics dieses vortrainierten Modells sind ziemlich gut. Diese Werte können als Vergleichsmaßstab für die selbst entwickelten Modelle dienen.\n\n## 4 Rekurrentes Neuronales Netzwerk\n\nDas LSTM (Long Short-Term Memory) Modell ist eine spezielle Form von Rekurrenten Neuronalen Netzwerken (RNNs), die besonders gut für die Verarbeitung von Sequenzdaten wie Texten geeignet ist.\n\n### 4.1 LSTM Modell erstellen\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(keras)\n\n# Datensatz bereinigt, ohne bereits Tokenisiert\ndata_all <- rbind(train_hate, test_hate)\n\n# Tokenisieren\nnum_words_train <- 1024\ntokens <- text_tokenizer(num_words = num_words_train,\n                            lower = TRUE) %>% \n  fit_text_tokenizer(data_all$tweet_clean)\n\n#train splitten für Kreuzvalidierung\nset.seed(142)\ntrain_split <- initial_split(train_hate, strata = class, prop = 0.8)\ndata_train <- training(train_split)\ndata_val <- testing(train_split)\n\n#maximale Länge eines Tweets\nmaxlen <- max(str_count(train_hate$tweet_clean, \"\\\\w+\")) + 1\n\n# prepare data for predictors x\ndata_train_x <- texts_to_sequences(tokens, data_train$tweet_clean) %>%\n  pad_sequences(maxlen = maxlen) \ndata_val_x <- texts_to_sequences(tokens, data_val$tweet_clean) %>%\n  pad_sequences(maxlen = maxlen) \ndata_test_x <- texts_to_sequences(tokens, test_hate$tweet_clean) %>%\n  pad_sequences(maxlen = maxlen)\n\n# Konvertieren in numerische Form\ndata_train <- data_train %>% \n  mutate(class = as.numeric(factor(class)))\n\ndata_val <- data_val %>% \n  mutate(class = as.numeric(factor(class)))\n\n# prepare data for target y\ndata_train_y <- to_categorical(data_train$class - 1, num_classes = 2)\ndata_val_y <- to_categorical(data_val$class - 1, num_classes = 2)\n\n#für Reproduzierbarkeit Zufallszahlengenerierung in R zu konfigurieren\nRNGkind(sample.kind = \"Rounding\")\ninitializer <- initializer_random_normal(seed = 100)\n\n## Model erstellen\nmodel_keras <- keras_model_sequential()\n\n# layer lstm 1 settings\nunit_lstm1 <- 256 #Neuronen\ndropout_lstm1 <- 0.5 #Dropout um Overfittig zu reduzieren\nrecurrent_dropout_lstm1 <- 0.5\n\n# layer lstm 2 settings\nunit_lstm2 <- 32\ndropout_lstm2 <- 0.5\nrecurrent_dropout_lstm2 <- 0.5\n\nmodel_keras %>%\n  layer_embedding(\n    name = \"input\",\n    input_dim = num_words_train,\n    input_length = maxlen,\n    output_dim = maxlen\n  ) %>%\n  layer_dropout(\n    name = \"embedding_dropout\",\n    rate = 0.6\n  ) %>%\n   # lstm1\n  layer_lstm(\n    name = \"lstm1\",\n    units = unit_lstm1,\n    dropout = dropout_lstm1,\n    recurrent_dropout = recurrent_dropout_lstm1,\n    return_sequences = TRUE\n  ) %>%\n  # lstm2\n  layer_lstm(\n    name = \"lstm2\",\n    units = unit_lstm2,\n    dropout = dropout_lstm2,\n    recurrent_dropout = recurrent_dropout_lstm2,\n    return_sequences = FALSE\n  ) %>%\n  # output layer\n  layer_dense(\n    name = \"output\",\n    units = 2,\n    activation = \"sigmoid\"\n  )\n\n# Compile Model\nmodel_keras %>% \n  compile(optimizer = optimizer_adam(learning_rate = 0.01),\n          metrics = \"accuracy\",\n          loss = \"binary_crossentropy\")\n\n# model summary\nsummary(model_keras)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nModel: \"sequential\"\n________________________________________________________________________________\n Layer (type)                       Output Shape                    Param #     \n================================================================================\n input (Embedding)                  (None, 33, 33)                  33792       \n embedding_dropout (Dropout)        (None, 33, 33)                  0           \n lstm1 (LSTM)                       (None, 33, 256)                 296960      \n lstm2 (LSTM)                       (None, 32)                      36992       \n output (Dense)                     (None, 2)                       66          \n================================================================================\nTotal params: 367810 (1.40 MB)\nTrainable params: 367810 (1.40 MB)\nNon-trainable params: 0 (0.00 Byte)\n________________________________________________________________________________\n```\n:::\n:::\n\n\n### 4.2 Model trainieren\n\n\n::: {.cell}\n\n```{.r .cell-code}\nepochs <- 10 #Durchläufe\nbatch_size <- 1024\n\n# fit the model\ntic()\nhistory <- model_keras %>%\n  fit(\n    data_train_x,\n    data_train_y,\n    batch_size = batch_size,\n    epochs = epochs,\n    verbose = 1,\n    validation_data = list(data_val_x, data_val_y))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nEpoch 1/10\n\n1/4 [======>.......................] - ETA: 16s - loss: 0.6931 - accuracy: 0.5225\n2/4 [==============>...............] - ETA: 1s - loss: 0.6563 - accuracy: 0.6309 \n3/4 [=====================>........] - ETA: 0s - loss: 0.6766 - accuracy: 0.6660\n4/4 [==============================] - ETA: 0s - loss: 0.6660 - accuracy: 0.6753\n4/4 [==============================] - 8s 828ms/step - loss: 0.6660 - accuracy: 0.6753 - val_loss: 0.5842 - val_accuracy: 0.7440\nEpoch 2/10\n\n1/4 [======>.......................] - ETA: 2s - loss: 0.5827 - accuracy: 0.7461\n2/4 [==============>...............] - ETA: 1s - loss: 0.5771 - accuracy: 0.7441\n3/4 [=====================>........] - ETA: 0s - loss: 0.5778 - accuracy: 0.7428\n4/4 [==============================] - ETA: 0s - loss: 0.5758 - accuracy: 0.7445\n4/4 [==============================] - 3s 687ms/step - loss: 0.5758 - accuracy: 0.7445 - val_loss: 0.5687 - val_accuracy: 0.7440\nEpoch 3/10\n\n1/4 [======>.......................] - ETA: 2s - loss: 0.5599 - accuracy: 0.7529\n2/4 [==============>...............] - ETA: 1s - loss: 0.5723 - accuracy: 0.7417\n3/4 [=====================>........] - ETA: 0s - loss: 0.5726 - accuracy: 0.7419\n4/4 [==============================] - ETA: 0s - loss: 0.5699 - accuracy: 0.7445\n4/4 [==============================] - 3s 681ms/step - loss: 0.5699 - accuracy: 0.7445 - val_loss: 0.5680 - val_accuracy: 0.7440\nEpoch 4/10\n\n1/4 [======>.......................] - ETA: 2s - loss: 0.5797 - accuracy: 0.7354\n2/4 [==============>...............] - ETA: 1s - loss: 0.5745 - accuracy: 0.7388\n3/4 [=====================>........] - ETA: 0s - loss: 0.5703 - accuracy: 0.7428\n4/4 [==============================] - ETA: 0s - loss: 0.5686 - accuracy: 0.7445\n4/4 [==============================] - 3s 669ms/step - loss: 0.5686 - accuracy: 0.7445 - val_loss: 0.5661 - val_accuracy: 0.7440\nEpoch 5/10\n\n1/4 [======>.......................] - ETA: 2s - loss: 0.5641 - accuracy: 0.7471\n2/4 [==============>...............] - ETA: 1s - loss: 0.5671 - accuracy: 0.7437\n3/4 [=====================>........] - ETA: 0s - loss: 0.5660 - accuracy: 0.7441\n4/4 [==============================] - ETA: 0s - loss: 0.5655 - accuracy: 0.7445\n4/4 [==============================] - 3s 672ms/step - loss: 0.5655 - accuracy: 0.7445 - val_loss: 0.5610 - val_accuracy: 0.7440\nEpoch 6/10\n\n1/4 [======>.......................] - ETA: 2s - loss: 0.5621 - accuracy: 0.7461\n2/4 [==============>...............] - ETA: 1s - loss: 0.5582 - accuracy: 0.7476\n3/4 [=====================>........] - ETA: 0s - loss: 0.5590 - accuracy: 0.7448\n4/4 [==============================] - ETA: 0s - loss: 0.5583 - accuracy: 0.7445\n4/4 [==============================] - 3s 683ms/step - loss: 0.5583 - accuracy: 0.7445 - val_loss: 0.5464 - val_accuracy: 0.7440\nEpoch 7/10\n\n1/4 [======>.......................] - ETA: 2s - loss: 0.5298 - accuracy: 0.7607\n2/4 [==============>...............] - ETA: 1s - loss: 0.5517 - accuracy: 0.7358\n3/4 [=====================>........] - ETA: 0s - loss: 0.5427 - accuracy: 0.7402\n4/4 [==============================] - ETA: 0s - loss: 0.5362 - accuracy: 0.7445\n4/4 [==============================] - 3s 672ms/step - loss: 0.5362 - accuracy: 0.7445 - val_loss: 0.5089 - val_accuracy: 0.7440\nEpoch 8/10\n\n1/4 [======>.......................] - ETA: 2s - loss: 0.4887 - accuracy: 0.7568\n2/4 [==============>...............] - ETA: 1s - loss: 0.4825 - accuracy: 0.7520\n3/4 [=====================>........] - ETA: 0s - loss: 0.4780 - accuracy: 0.7454\n4/4 [==============================] - ETA: 0s - loss: 0.4749 - accuracy: 0.7448\n4/4 [==============================] - 3s 673ms/step - loss: 0.4749 - accuracy: 0.7448 - val_loss: 0.4099 - val_accuracy: 0.7667\nEpoch 9/10\n\n1/4 [======>.......................] - ETA: 2s - loss: 0.4054 - accuracy: 0.7646\n2/4 [==============>...............] - ETA: 1s - loss: 0.4114 - accuracy: 0.7974\n3/4 [=====================>........] - ETA: 0s - loss: 0.4231 - accuracy: 0.7923\n4/4 [==============================] - ETA: 0s - loss: 0.4277 - accuracy: 0.7922\n4/4 [==============================] - 3s 673ms/step - loss: 0.4277 - accuracy: 0.7922 - val_loss: 0.4019 - val_accuracy: 0.8381\nEpoch 10/10\n\n1/4 [======>.......................] - ETA: 2s - loss: 0.3783 - accuracy: 0.8506\n2/4 [==============>...............] - ETA: 1s - loss: 0.3591 - accuracy: 0.8809\n3/4 [=====================>........] - ETA: 0s - loss: 0.3648 - accuracy: 0.8818\n4/4 [==============================] - ETA: 0s - loss: 0.3654 - accuracy: 0.8819\n4/4 [==============================] - 3s 679ms/step - loss: 0.3654 - accuracy: 0.8819 - val_loss: 0.3678 - val_accuracy: 0.8690\n```\n:::\n\n```{.r .cell-code}\n# history plot\nplot(history)\n```\n\n::: {.cell-output-display}\n![](Abgabe_files/figure-html/unnamed-chunk-19-1.png){width=672}\n:::\n\n```{.r .cell-code}\ntoc()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n34.05 sec elapsed\n```\n:::\n:::\n\n\n### 4.3 Modell Evaluation\n\nDie Anwendung des Models auf die Datensätze train, validation und test ergibt nachstehende Metriken.\n\n::: panel-tabset\n## train-Datensatz\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\" code-summary=\"Show the Code\"}\n# vorhersagen auf train-Datensatz\ndata_train_pred <- model_keras %>%\n  predict(data_train_x) %>%\n  k_argmax() %>%\n  as.vector()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n105/105 - 2s - 2s/epoch - 17ms/step\n```\n:::\n\n```{.r .cell-code  code-fold=\"true\" code-summary=\"Show the Code\"}\n#Metriken train-Datensatz\nconf.matrix <-  confusionMatrix(\n  factor(data_train_pred, labels = c(\"no\", \"yes\")),\n  factor(data_train$class, labels = c(\"no\", \"yes\")),\n  positive = \"yes\"\n)\n\n#Anzeigen\nconf.matrix$byClass %>% round(digits = 3) * 100\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n         Sensitivity          Specificity       Pos Pred Value \n                97.3                 75.1                 91.9 \n      Neg Pred Value            Precision               Recall \n                90.4                 91.9                 97.3 \n                  F1           Prevalence       Detection Rate \n                94.5                 74.4                 72.4 \nDetection Prevalence    Balanced Accuracy \n                78.8                 86.2 \n```\n:::\n:::\n\n\n## Validation-Datensatz\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\" code-summary=\"Show the Code\"}\n# vorhersagen auf data_val (= Datensatz für Kreuzvalidierung)\ndata_val_pred <- model_keras %>%\n  predict(data_val_x) %>%\n  k_argmax() %>%\n  as.vector()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n27/27 - 0s - 340ms/epoch - 13ms/step\n```\n:::\n\n```{.r .cell-code  code-fold=\"true\" code-summary=\"Show the Code\"}\n#Metriken für Teil von train-Datensatz (data_val)\nconf.matrix <-  confusionMatrix(\n  factor(data_val_pred, labels = c(\"no\", \"yes\")),\n  factor(data_val$class, labels = c(\"no\", \"yes\")),\n  positive = \"yes\"\n)\n\n#Anzeigen\nconf.matrix$byClass %>% round(digits = 3) * 100\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n         Sensitivity          Specificity       Pos Pred Value \n                95.0                 63.3                 88.3 \n      Neg Pred Value            Precision               Recall \n                81.4                 88.3                 95.0 \n                  F1           Prevalence       Detection Rate \n                91.5                 74.4                 70.7 \nDetection Prevalence    Balanced Accuracy \n                80.1                 79.1 \n```\n:::\n:::\n\n\n## test-Datensatz\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\" code-summary=\"Show the Code\"}\n# vorhersagen auf test-Datensatz\ndata_test_pred <- model_keras %>%\n  predict(data_test_x) %>%\n  k_argmax() %>%\n  as.vector()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n44/44 - 1s - 535ms/epoch - 12ms/step\n```\n:::\n\n```{.r .cell-code  code-fold=\"true\" code-summary=\"Show the Code\"}\n#Metriken test-Datensatz\nconf.matrix <-  confusionMatrix(\n  factor(data_test_pred, labels = c(\"no\", \"yes\")),\n  factor(test_hate$class, labels = c(\"no\", \"yes\")),\n  positive = \"yes\"\n)\n\n#Anzeigen\nconf.matrix$byClass %>% round(digits = 3) * 100\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n         Sensitivity          Specificity       Pos Pred Value \n                95.8                 64.2                 88.6 \n      Neg Pred Value            Precision               Recall \n                83.9                 88.6                 95.8 \n                  F1           Prevalence       Detection Rate \n                92.1                 74.4                 71.3 \nDetection Prevalence    Balanced Accuracy \n                80.4                 80.0 \n```\n:::\n:::\n\n:::\n\nDie Hate Speech Kalassifikation fällt schlechter aus als beim Hugging Face Modell. Dennoch kann man zufrieden sein.\n\n## 5 Modell mit Tidymodels\n\nNun werden Modelle mit Tidymodels erstellt. Verwendet werden hierfür Random Forest, Nearest Neighbor und XGBoost.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmod_rand <- rand_forest(mode = \"classification\")\nmod_neighbor <- nearest_neighbor(mode = \"classification\")\nmod_xgb <- boost_tree(mode = \"classification\")\n\n## Mit Tuning\nmod_randT <-\n  rand_forest(mtry = tune(),\n              min_n = tune(),\n              trees = 1000, #guter Wert (keine Veränderungen durch tunen)\n              mode = \"classification\") %>% \n  set_engine(\"ranger\", num.threads = 4)\n\nmod_neighborT <- \n  nearest_neighbor(neighbors = tune(),\n                   mode = \"classification\")\n\nmod_xgbT <- \n   boost_tree(min_n = tune(), \n              trees = 1000,\n              learn_rate = tune()) %>% \n  set_mode(\"classification\") %>% \n  set_engine(\"xgboost\", nthreads = parallel::detectCores()-1)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n#Daten (ohne Tokens) und Lexikon laden\ntrain_mod <- train_hate %>% \n  select(-tweet)\ntest_mod <- test_hate %>% \n  select(-tweet)\n\nInsults <- read_excel(insult_pfad)\nnames(Insults) <- \"word\" #get_sentiment braucht Bezeichnung word\nInsults$value <- 1\n\n#recipe\nrec <- recipe(class ~ ., data = train_mod) %>%  \n  update_role(id, new_role = \"id\") %>% \n  update_role(tweet_clean, new_role = \"ignore\") %>% #Spalte wird kein Prädiktor\n  step_mutate(insult = get_sentiment(tweet_clean,  \n                                    method = \"custom\",\n                                    lexicon = Insults)) %>% \n  step_textfeature(tweet_clean, keep_original_cols = TRUE) %>% \n  step_nzv(all_predictors()) %>% #textfeature erzeugt viele sinnlose Spalten\n  step_tokenize(tweet_clean) %>%\n  step_stopwords(tweet_clean, keep = FALSE) %>%\n  step_tokenfilter(tweet_clean, max_tokens = 1e3) %>% \n  step_tfidf(tweet_clean, keep_original_cols = TRUE) %>% \n  step_nzv(all_predictors()) %>%  #(zweimal, um sofort Spalten zu entfernen)\n  step_normalize(all_numeric_predictors())\n\n\nrec_prepped <- prep(rec)\ndata_prep <- bake(rec_prepped, new_data = NULL)\n# Erkenntisse bake: tfidf wird durch step_nzv komplett entfernt\n\n\n#Kreuzvalidierung\nset.seed(42)\nkv <- vfold_cv(train_mod, strata = class)\n```\n:::\n\n\nDie Leistung der trainierten Modelle fällt wie folgt aus.\n\n### 5.1 Ohne Tuning\n\n::: panel-tabset\n## Random Forest\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\" code-summary=\"Show the Code\"}\nwf_rand <-\n  workflow() %>% \n  add_model(mod_rand) %>% \n  add_recipe(rec)\n\ntic()\nfit_rand <-\n  fit(wf_rand,\n      data = train_mod)\ntoc()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n9.61 sec elapsed\n```\n:::\n\n```{.r .cell-code  code-fold=\"true\" code-summary=\"Show the Code\"}\n#Vorhersage\ntic()\npreds <-\n  predict(fit_rand, new_data = test_mod)\ntoc()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n1.54 sec elapsed\n```\n:::\n\n```{.r .cell-code  code-fold=\"true\" code-summary=\"Show the Code\"}\n#Test\nVorgersage <-\n  test_mod %>%  \n  bind_cols(preds)\n\nmy_metrics <- metric_set(accuracy, f_meas)\nmy_metrics(Vorgersage,\n           truth = as.factor(class), #estimate ist factor, also class umwandeln\n           estimate = .pred_class)\n```\n\n::: {.cell-output-display}\n`````{=html}\n<div data-pagedtable=\"false\">\n  <script data-pagedtable-source type=\"application/json\">\n{\"columns\":[{\"label\":[\".metric\"],\"name\":[1],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\".estimator\"],\"name\":[2],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\".estimate\"],\"name\":[3],\"type\":[\"dbl\"],\"align\":[\"right\"]}],\"data\":[{\"1\":\"accuracy\",\"2\":\"binary\",\"3\":\"0.7576841\"},{\"1\":\"f_meas\",\"2\":\"binary\",\"3\":\"0.3123732\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\n  </script>\n</div>\n`````\n:::\n:::\n\n\n## Nearest Neighbor\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\" code-summary=\"Show the Code\"}\nwf_n <-\n  workflow() %>% \n  add_model(mod_neighbor) %>% \n  add_recipe(rec)\n\ntic()\nfit_n <-\n  fit(wf_n,\n      data = train_mod)\ntoc()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n8.94 sec elapsed\n```\n:::\n\n```{.r .cell-code  code-fold=\"true\" code-summary=\"Show the Code\"}\n#Vorhersage\ntic()\npreds <-\n  predict(fit_n, new_data = test_mod)\ntoc()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n1.47 sec elapsed\n```\n:::\n\n```{.r .cell-code  code-fold=\"true\" code-summary=\"Show the Code\"}\n#Test\nVorgersage <-\n  test_mod %>%  \n  bind_cols(preds)\n\nmy_metrics <- metric_set(accuracy, f_meas)\nmy_metrics(Vorgersage,\n           truth = as.factor(class), #estimate ist factor, also class umwandeln\n           estimate = .pred_class)\n```\n\n::: {.cell-output-display}\n`````{=html}\n<div data-pagedtable=\"false\">\n  <script data-pagedtable-source type=\"application/json\">\n{\"columns\":[{\"label\":[\".metric\"],\"name\":[1],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\".estimator\"],\"name\":[2],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\".estimate\"],\"name\":[3],\"type\":[\"dbl\"],\"align\":[\"right\"]}],\"data\":[{\"1\":\"accuracy\",\"2\":\"binary\",\"3\":\"0.7119371\"},{\"1\":\"f_meas\",\"2\":\"binary\",\"3\":\"0.3712949\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\n  </script>\n</div>\n`````\n:::\n:::\n\n\n## XGBoost\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\" code-summary=\"Show the Code\"}\nwf_boost <-\n  workflow() %>% \n  add_model(mod_xgb) %>% \n  add_recipe(rec)\n\ntic()\nfit_boost <-\n  fit(wf_boost,\n      data = train_mod)\ntoc()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n8.31 sec elapsed\n```\n:::\n\n```{.r .cell-code  code-fold=\"true\" code-summary=\"Show the Code\"}\n#Vorhersage\ntic()\npreds <-\n  predict(fit_boost, new_data = test_mod)\ntoc()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n1.37 sec elapsed\n```\n:::\n\n```{.r .cell-code  code-fold=\"true\" code-summary=\"Show the Code\"}\n#Test\nVorgersage <-\n  test_mod %>%  \n  bind_cols(preds)\n\nmy_metrics <- metric_set(accuracy, f_meas)\nmy_metrics(Vorgersage,\n           truth = as.factor(class), #estimate ist factor, also class umwandeln\n           estimate = .pred_class)\n```\n\n::: {.cell-output-display}\n`````{=html}\n<div data-pagedtable=\"false\">\n  <script data-pagedtable-source type=\"application/json\">\n{\"columns\":[{\"label\":[\".metric\"],\"name\":[1],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\".estimator\"],\"name\":[2],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\".estimate\"],\"name\":[3],\"type\":[\"dbl\"],\"align\":[\"right\"]}],\"data\":[{\"1\":\"accuracy\",\"2\":\"binary\",\"3\":\"0.7705504\"},{\"1\":\"f_meas\",\"2\":\"binary\",\"3\":\"0.2913907\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\n  </script>\n</div>\n`````\n:::\n:::\n\n:::\n\nOhne Tuning erhält man bereits gute Accuracy-Werte, welche gut mit dem LSTM Modell mithalten können. Die Rechenzeit von unter 10 Sekunden ist ebenfalls sehr angenehm.\n\nMal sehen, ob sich die Modelle durch Tuning verbessern.\n\n### 5.2 Mit Tuning\n\n::: panel-tabset\n## Random Forest\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\" code-summary=\"Show the Code\"}\nwf_randT <-\n  workflow() %>% \n  add_model(mod_randT) %>% \n  add_recipe(rec)\n\ntic()\nset.seed(42)\nrf_tune <- tune_grid(object = wf_randT, resamples = kv, grid = 5,\n                         control = control_grid(save_workflow = TRUE))\ntoc()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n154.45 sec elapsed\n```\n:::\n\n```{.r .cell-code  code-fold=\"true\" code-summary=\"Show the Code\"}\n#Ergebnisse\nbest_model_rf <- fit_best(rf_tune)\n\nrf_aufTest <- last_fit(best_model_rf, train_test_split)\nrf_aufTest %>% collect_metrics()\n```\n\n::: {.cell-output-display}\n`````{=html}\n<div data-pagedtable=\"false\">\n  <script data-pagedtable-source type=\"application/json\">\n{\"columns\":[{\"label\":[\".metric\"],\"name\":[1],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\".estimator\"],\"name\":[2],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\".estimate\"],\"name\":[3],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\".config\"],\"name\":[4],\"type\":[\"chr\"],\"align\":[\"left\"]}],\"data\":[{\"1\":\"accuracy\",\"2\":\"binary\",\"3\":\"0.7712652\",\"4\":\"Preprocessor1_Model1\"},{\"1\":\"roc_auc\",\"2\":\"binary\",\"3\":\"0.6498251\",\"4\":\"Preprocessor1_Model1\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\n  </script>\n</div>\n`````\n:::\n:::\n\n\n## Nearest Neighbor\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\" code-summary=\"Show the Code\"}\nwf_knnT <-\n  workflow() %>% \n  add_model(mod_neighborT) %>% \n  add_recipe(rec)\n\ntic()\nset.seed(42)\nknn_tune <- tune_grid(object = wf_knnT, resamples = kv, grid = 5,\n                         control = control_grid(save_workflow = TRUE))\ntoc()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n85.89 sec elapsed\n```\n:::\n\n```{.r .cell-code  code-fold=\"true\" code-summary=\"Show the Code\"}\n#Ergebnisse\nbest_model_knn <- fit_best(knn_tune)\n\nknn_aufTest <- last_fit(best_model_knn, train_test_split)\nknn_aufTest %>% collect_metrics()\n```\n\n::: {.cell-output-display}\n`````{=html}\n<div data-pagedtable=\"false\">\n  <script data-pagedtable-source type=\"application/json\">\n{\"columns\":[{\"label\":[\".metric\"],\"name\":[1],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\".estimator\"],\"name\":[2],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\".estimate\"],\"name\":[3],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\".config\"],\"name\":[4],\"type\":[\"chr\"],\"align\":[\"left\"]}],\"data\":[{\"1\":\"accuracy\",\"2\":\"binary\",\"3\":\"0.7476769\",\"4\":\"Preprocessor1_Model1\"},{\"1\":\"roc_auc\",\"2\":\"binary\",\"3\":\"0.6470170\",\"4\":\"Preprocessor1_Model1\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\n  </script>\n</div>\n`````\n:::\n:::\n\n\n## XGBoost\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\" code-summary=\"Show the Code\"}\nwf_xgbT <-\n  workflow() %>% \n  add_model(mod_xgbT) %>% \n  add_recipe(rec)\n\ntic()\nset.seed(42)\nxgb_tune <- tune_grid(object = wf_xgbT, resamples = kv, grid = 5,\n                         control = control_grid(save_workflow = TRUE))\ntoc()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n205.83 sec elapsed\n```\n:::\n\n```{.r .cell-code  code-fold=\"true\" code-summary=\"Show the Code\"}\n#Ergebnisse\nbest_model_xgb <- fit_best(xgb_tune)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[21:39:26] WARNING: src/learner.cc:767: \nParameters: { \"nthreads\" } are not used.\n```\n:::\n\n```{.r .cell-code  code-fold=\"true\" code-summary=\"Show the Code\"}\nxgb_aufTest <- last_fit(best_model_xgb, train_test_split)\nxgb_aufTest %>% collect_metrics()\n```\n\n::: {.cell-output-display}\n`````{=html}\n<div data-pagedtable=\"false\">\n  <script data-pagedtable-source type=\"application/json\">\n{\"columns\":[{\"label\":[\".metric\"],\"name\":[1],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\".estimator\"],\"name\":[2],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\".estimate\"],\"name\":[3],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\".config\"],\"name\":[4],\"type\":[\"chr\"],\"align\":[\"left\"]}],\"data\":[{\"1\":\"accuracy\",\"2\":\"binary\",\"3\":\"0.7691208\",\"4\":\"Preprocessor1_Model1\"},{\"1\":\"roc_auc\",\"2\":\"binary\",\"3\":\"0.6595734\",\"4\":\"Preprocessor1_Model1\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\n  </script>\n</div>\n`````\n:::\n:::\n\n:::\n\nWenn man die Accuracy zwischen den optimierten und den nicht optimierten Modellen vergleicht, sind diese kaum besser oder gleich gut. Folglich kann man sich in diesem Fall den erhöhten Zeitaufwand, der durch das Tuning entsteht, sparen.\n\n## 6 Fazit\n\nDie Klassifikation von Hate Speech in Textnachrichten stellt eine komplexe und anspruchsvolle Aufgabe dar. Durch den Einsatz von maschinellen Lernmodellen und Textanalysemethoden konnten jedoch vielversprechende Ergebnisse erzielt werden.\n\nInsgesamt kann festgestellt werden, dass das Hugging Face Modell am besten abschnitt. Dennoch konnten die selbst trainierten Modelle gut mithalten. Die nicht optimierten Tidymodels Modelle sind mit einer Rechenzeit von ca. 10 Sekunden extrem schnell. Wenn man schnelle Ergebnisse möchte, sind diese auf jeden Fall zu Empfehlen.\n\nDer Post zeigt, dass der Einsatz von Machine-Learning-Techniken die Bekämpfung von Hassrede in den sozialen Medien unterstützen kann. Es bleibt jedoch eine Herausforderung, diese Technologie verantwortungsvoll einzusetzen und sicherzustellen, dass die Meinungsfreiheit respektiert wird und keine unbeabsichtigten Nebenwirkungen entstehen.\n",
    "supporting": [
      "Abgabe_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "<script src=\"../../site_libs/htmlwidgets-1.6.2/htmlwidgets.js\"></script>\r\n<script src=\"../../site_libs/d3-4.5.0/d3.min.js\"></script>\r\n<script src=\"../../site_libs/forceNetwork-binding-0.4/forceNetwork.js\"></script>\r\n<link href=\"../../site_libs/pagedtable-1.1/css/pagedtable.css\" rel=\"stylesheet\" />\r\n<script src=\"../../site_libs/pagedtable-1.1/js/pagedtable.js\"></script>\r\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}