[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Viel Spaß auf meinem persönlichen Data Science Blog."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "blognilsbiller",
    "section": "",
    "text": "Hate Speech Klassifikation\n\n\n\n\n\n\n\nTextanalyse\n\n\nTidymodels\n\n\nKlassifikation\n\n\nTransformers\n\n\nNeuronale Netze\n\n\n\n\n\n\n\n\n\n\n\nFeb 9, 2024\n\n\nNils Biller\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/DataScience2_Abgabe/Abgabe.html",
    "href": "posts/DataScience2_Abgabe/Abgabe.html",
    "title": "Hate Speech Klassifikation",
    "section": "",
    "text": "Die Klassifikation von Hassrede in Textnachrichten ist entscheidend, um die Auswirkungen von negativem Verhalten in sozialen Medien zu verstehen und einzudämmen. Durch die Entwicklung von Algorithmen und Modellen, die automatisch Hassrede erkennen, können wir potenziell schädliche Inhalte identifizieren und bekämpfen. Dieser Post bietet einen Überblick über einige Ansätze und Techniken zur Erkennung von Hassrede.\nZur Demonstration der Modelle wird ein Datensatz mit Twitternachrichten verwendet.\n\n\n\nlibrary(tidyverse)\nlibrary(tidytext)\nlibrary(tidymodels)\nlibrary(wordcloud)\nlibrary(tictoc)\nlibrary(tm) #für Corpus\nlibrary(caret) #für Konfusionsmatrix\nlibrary(textrecipes) #für u.a. step_textfeature\nlibrary(syuzhet) #Stimmungsanalyse\nlibrary(readxl) #für Exeldateien\nlibrary(xgboost)\n\n\n\n\n\nd_hate <- read_csv(pfad)\n\n\n\n\n## Tweets bereinigen\nclean.text = function(x)\n{\n  x = tolower(x)                 # alles in Kleinbuchstaben umwandeln\n  x = gsub(\"rt\", \"\", x)          # rt entfernen\n  x = gsub(\"@\\\\w+\", \"\", x)       # alle @ + Namen entfernen\n  x = gsub(\"[[:punct:]]\", \"\", x) # Satzzeichen entfernen\n  x = gsub(\"[[:digit:]]\", \"\", x) # Zahlen entfernen\n  x = gsub(\"http\\\\w+\", \"\", x)    # Links entfernen\n  x = gsub(\"http\", \"\", x)        # hhtp entfernen wenn es alleine steht\n  x = gsub(\"[ |\\t]{2,}\", \" \", x) # doppelte Leerzeichen und Tabs entfernen\n  x = gsub(\"^ \", \"\", x)          # Leerzeichen am Anfang entfernen\n  x = gsub(\" $\", \"\", x)          # Leerzeichen am Ende entfernen\n\n  return(x)\n}\n\n#Funktion anwenden\ndf_hate <- d_hate %>% \n  mutate(tweet_clean = tweet) %>% \n  mutate(tweet_clean = clean.text(tweet_clean))\n\n#Splitten\nset.seed(142)\ntrain_test_split <- initial_split(df_hate, strata = class)\ntrain_hate <- training(train_test_split)\ntest_hate <- testing(train_test_split)\n\n\n\n\nNachdem die Tweets nun bereinigt sind, können die Stoppwörter entfernt werden, da sie in der Regel keine besondere Rolle bei der Interpretation eines Satzes spielen.\n\n# unnest\ndf_hate <- df_hate %>%\n  mutate(words = tweet_clean) %>% \n  unnest_tokens(words, words)\n\n## die 15 häufigsten Wörter\ndf_hate %>%\n  count(words, sort = TRUE) %>%\n  top_n(15) %>%\n  mutate(words = reorder(words, n)) %>%\n  ggplot(aes(x = words, y = n)) +\n  geom_col() +\n  xlab(NULL) +\n  coord_flip() +\n      labs(x = \"Wort\",\n      y = \"Anzahl\",\n      title = \"Die 15 häufigsten Wörter\") +\n  theme_minimal()\n\n\n\n\nDie neun häufigsten Wörter sind Stoppwörter. Dies demonstriert, dass es sinnvoll ist diese zu entfernen.\n\n# Stoppwortentfernung\ndata(\"stop_words\")\n\ndf_hate <- df_hate %>%\n  anti_join(stop_words, by = c(\"words\" = \"word\"))\n\n\n\nShow the Code\n## die 15 häufigsten Wörter\ndf_hate %>%\n  count(words, sort = TRUE) %>%\n  top_n(15) %>%\n  mutate(words = reorder(words, n)) %>%\n  ggplot(aes(x = words, y = n)) +\n  geom_col() +\n  xlab(NULL) +\n  coord_flip() +\n      labs(x = \"Wort\",\n      y = \"Anzahl\",\n      title = \"Die 15 häufigsten Wörter\") +\n  theme_minimal()\n\n\n\n\n\nDas Entfernen der Stoppwörter war erfolgreich. Das nun mit Abstand häufigste Wort ist “trash”.\n\n\n\n\nUm einen Überblick über die Tweets zu erhalten, werden diese im Folgenden grafisch analysiert.\n\n\n\n# Converting tweets to ASCII, um Verarbeitung zu garantieren\ntweets <- df_hate %>% \n  select(tweet_clean)\ntweets <- iconv(tweets, from=\"UTF-8\", to=\"ASCII\", sub=\"\")\n\n#Sentiments erkennen\nsentiments <- get_nrc_sentiment((tweets))\n\n#Score erstellen\nsentimentscores<-data.frame(colSums(sentiments[,]))\n\n#Spalte umbenennen\nnames(sentimentscores) <- \"Score\"\n\n#neue Spalte \"sentiment\"\nsentimentscores <- cbind(\"sentiment\"=rownames(sentimentscores),sentimentscores)\nrownames(sentimentscores) <- NULL\n\n#plotten\nggplot(data=sentimentscores,aes(x=sentiment,y=Score))+\n  geom_bar(aes(fill=sentiment),stat = \"identity\")+\n  theme(legend.position=\"none\")+\n  xlab(\"Sentiments\")+ylab(\"Scores\")+\n  ggtitle(\"Stimmung in den Tweets\")+\n  theme_minimal()+\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n\n\n\n\nDiese Graphik bietet einen schönen Überblick über die Stimmung in den Tweets. Positive und Negative Tweets sind relativ ausgeglichen. Jede Stimmung ist ähnlich oft vorhanden.\n\n\n\n\n# Worttoken umwandeln\ntweets <- Corpus(VectorSource(df_hate$words))\n\nset.seed(42)\nwordcloud(tweets, min.freq = 1, max.words = 100, scale = c(2.2,1),\n          colors=brewer.pal(8, \"Dark2\"), random.color = T, random.order = F,\n          rot.per=0.25)\n\n\n\n\nHier sieht man die häufigsten Wörter aus den Tweets.\nUnterscheidet man zwischen Tweets die als Hate Speech kassifiziert sind und denen die kein Hate Speech sind, bekommt man folgende Wordclouds.\n\nHate SpeechNo Hate Speech\n\n\n\n\nShow the Code\n## Wortwolke Hate Speech\n\n# Worttoken umwandeln\ntweets <- df_hate %>% \n  filter(class == \"hate speech\")\n\ntweets <- Corpus(VectorSource(tweets$words))\n\n#Cloud erstellen\nset.seed(42)\nwordcloud(tweets, min.freq = 1, max.words = 100, scale = c(2.2,1),\n          colors=brewer.pal(8, \"Dark2\"), random.color = T, random.order = F,\n          rot.per=0.25)\n\n\n\n\n\n\n\n\n\nShow the Code\n## Wortwolke Non Hate Speech\n\n# Worttoken umwandeln\ntweets <- df_hate %>% \n  filter(class != \"hate speech\")\n\ntweets <- Corpus(VectorSource(tweets$words))\n\n#Cloud erstellen\nset.seed(42)\nwordcloud(tweets, min.freq = 1, max.words = 100, scale = c(2.2,1),\n          colors=brewer.pal(8, \"Dark2\"), random.color = T, random.order = F,\n          rot.per=0.25)\n\n\n\n\n\n\n\n\nAuffällig ist, dass bei No Hate Speech auch das Wort “hoe” vorkommt (zu sehen rechts unten Mitte).\nErklärt werden kann dies mit folgendem Tweet.\n\n\nShow the Code\ntweets <- df_hate %>% \n  filter(class != \"hate speech\") %>% \n  filter(words == \"hoe\")\n\ntweets[5, \"tweet_clean\"]\n\n\n\n\n  \n\n\n\nHomonyme wie dieses, könnten zu Fehlern in der Vorhersage führen.\n\n\n\nDie am häufigsten vorkommenden Wortpaare sind in folgender interaktiven Graphik zu sehen.\nNavigation: linke Maustaste: bewegen; Mausrad: zoomen\n\n\nShow the Code\n## Bigram interaktiv\n#bigram Tabelle\nbigram <- df_hate %>% \n  unnest_tokens(\n    input = tweet_clean, \n    output = bigram, \n    token = 'ngrams', \n    n = 2\n  ) %>% \n  filter(! is.na(bigram))\n\nbigram %<>% \n  separate(col = bigram, into = c('word1', 'word2'), sep = ' ') %>% \n  filter(! word1 %in% stop_words$word) %>% \n  filter(! word2 %in% stop_words$word) %>% \n  filter(! is.na(word1)) %>% \n  filter(! is.na(word2)) \n\nbigram_count <- bigram %>% \n  count(word1, word2, sort = TRUE) %>% \n  rename(weight = n)\n\n\n# bigram erstellen\nlibrary(networkD3)\nlibrary(igraph)\nlibrary(magrittr)\n\nthreshold <- 50\n\nnetwork <-  bigram_count %>%\n  filter(weight > threshold) %>%\n  graph_from_data_frame(directed = FALSE)\n\n# Anzahl der Kanten\nV(network)$degree <- strength(graph = network)\n# Gewichtung\nE(network)$width <- E(network)$weight/max(E(network)$weight)\n\n# Create networkD3 object.\nnetwork.D3 <- igraph_to_networkD3(g = network)\n# Define node size.\nnetwork.D3$nodes %<>% mutate(Degree = (1E-2)*V(network)$degree)\n# Define color group\nnetwork.D3$nodes %<>% mutate(Group = 1)\n# Define edges width. \nnetwork.D3$links$Width <- 10*E(network)$width\n\nforceNetwork(\n  Links = network.D3$links, \n  Nodes = network.D3$nodes, \n  Source = 'source', \n  Target = 'target',\n  NodeID = 'name',\n  Group = 'Group', \n  opacity = 0.9,\n  Value = 'Width',\n  Nodesize = 'Degree', \n  # We input a JavaScript function.\n  linkWidth = JS(\"function(d) { return Math.sqrt(d.value); }\"), \n  fontSize = 12,\n  zoom = TRUE, \n  opacityNoHover = 1\n)\n\n\n\n\n\n\n\n\n\n\nUm Hate Speech in Textnachrichten zu klassifizieren, können vortrainierte Modelle wie das “roberta” Modell von Hugging Face verwendet werden. Dieses Modell wurde auf Grundlage von 40.000 Textnachrichten entwickelt.\n\nlibrary(reticulate)\nuse_virtualenv(\"~/Studium/blognilsbiller/VirtualEnv\")\n\n\nfrom transformers import pipeline\n\nWARNING:tensorflow:From C:\\Users\\NILS~1.DES\\DOCUME~1\\Studium\\BLOGNI~1\\VIRTUA~1\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n\nimport tensorflow\n\n\nclassifier = pipeline(\"text-classification\", model=\"facebook/roberta-hate-speech-dynabench-r4-target\")\n\n\ntweets <- test_hate$tweet_clean\n\n\ntweets = r.tweets\nresults = classifier(tweets)\n\n\nresult <- py$results\n\n# Extrahieren der Labels\nlabels <- lapply(result, function(element) element$label)\n\n# Zusammenführen der Ergebnisse mit dem ursprünglichen Datensatz\ntweets_zsm <- bind_cols(test_hate, pred = unlist(labels))\n\n# Umwandeln in Faktoren und Umbenennen der Vorhersagen\ntweets_zsm <- tweets_zsm %>%\n  mutate(pred = factor(case_when(pred == \"hate\" ~ \"hate speech\",\n                                 pred == \"nothate\" ~ \"other\")))\n\n\n#Vorhersage\nmy_metrics <- metric_set(accuracy, f_meas)\nmy_metrics(tweets_zsm,\n           truth = as.factor(class), #estimate ist factor, also class anpassen\n           estimate = pred)\n\n\n\n  \n\n\n\nDie Metrics dieses vortrainierten Modells sind ziemlich gut. Diese Werte können als Vergleichsmaßstab für die selbst entwickelten Modelle dienen.\n\n\n\nDas LSTM (Long Short-Term Memory) Modell ist eine spezielle Form der Rekurrenten Neuronalen Netzwerke (RNNs), die besonders gut für die Verarbeitung von Sequenzdaten wie Texten geeignet sind.\n\n\n\nlibrary(keras)\n\n# Datensatz bereinigt, ohne bereits Tokenisiert\ndata_all <- rbind(train_hate, test_hate)\n\n# Tokenisieren\nnum_words_train <- 1024\ntokens <- text_tokenizer(num_words = num_words_train,\n                            lower = TRUE) %>% \n  fit_text_tokenizer(data_all$tweet_clean)\n\n#train splitten für Kreuzvalidierung\nset.seed(142)\ntrain_split <- initial_split(train_hate, strata = class, prop = 0.8)\ndata_train <- training(train_split)\ndata_val <- testing(train_split)\n\n#maximale Länge eines Tweets\nmaxlen <- max(str_count(train_hate$tweet_clean, \"\\\\w+\")) + 1\n\n# prepare data for predictors x\ndata_train_x <- texts_to_sequences(tokens, data_train$tweet_clean) %>%\n  pad_sequences(maxlen = maxlen) \ndata_val_x <- texts_to_sequences(tokens, data_val$tweet_clean) %>%\n  pad_sequences(maxlen = maxlen) \ndata_test_x <- texts_to_sequences(tokens, test_hate$tweet_clean) %>%\n  pad_sequences(maxlen = maxlen)\n\n# Konvertieren in numerische Form\ndata_train <- data_train %>% \n  mutate(class = as.numeric(factor(class)))\n\ndata_val <- data_val %>% \n  mutate(class = as.numeric(factor(class)))\n\n# prepare data for target y\ndata_train_y <- to_categorical(data_train$class - 1, num_classes = 2)\ndata_val_y <- to_categorical(data_val$class - 1, num_classes = 2)\n\n#für Reproduzierbarkeit Zufallszahlengenerierung in R zu konfigurieren\nRNGkind(sample.kind = \"Rounding\")\ninitializer <- initializer_random_normal(seed = 100)\n\n## Model erstellen\nmodel_keras <- keras_model_sequential()\n\n# layer lstm 1 settings\nunit_lstm1 <- 256 #Neuronen\ndropout_lstm1 <- 0.5 #Dropout um Overfittig zu reduzieren\nrecurrent_dropout_lstm1 <- 0.5\n\n# layer lstm 2 settings\nunit_lstm2 <- 32\ndropout_lstm2 <- 0.5\nrecurrent_dropout_lstm2 <- 0.5\n\nmodel_keras %>%\n  layer_embedding(\n    name = \"input\",\n    input_dim = num_words_train,\n    input_length = maxlen,\n    output_dim = maxlen\n  ) %>%\n  layer_dropout(\n    name = \"embedding_dropout\",\n    rate = 0.6\n  ) %>%\n   # lstm1\n  layer_lstm(\n    name = \"lstm1\",\n    units = unit_lstm1,\n    dropout = dropout_lstm1,\n    recurrent_dropout = recurrent_dropout_lstm1,\n    return_sequences = TRUE\n  ) %>%\n  # lstm2\n  layer_lstm(\n    name = \"lstm2\",\n    units = unit_lstm2,\n    dropout = dropout_lstm2,\n    recurrent_dropout = recurrent_dropout_lstm2,\n    return_sequences = FALSE\n  ) %>%\n  # output layer\n  layer_dense(\n    name = \"output\",\n    units = 2,\n    activation = \"sigmoid\"\n  )\n\n# Compile Model\nmodel_keras %>% \n  compile(optimizer = optimizer_adam(learning_rate = 0.01),\n          metrics = \"accuracy\",\n          loss = \"binary_crossentropy\")\n\n# model summary\nsummary(model_keras)\n\nModel: \"sequential\"\n________________________________________________________________________________\n Layer (type)                       Output Shape                    Param #     \n================================================================================\n input (Embedding)                  (None, 33, 33)                  33792       \n embedding_dropout (Dropout)        (None, 33, 33)                  0           \n lstm1 (LSTM)                       (None, 33, 256)                 296960      \n lstm2 (LSTM)                       (None, 32)                      36992       \n output (Dense)                     (None, 2)                       66          \n================================================================================\nTotal params: 367810 (1.40 MB)\nTrainable params: 367810 (1.40 MB)\nNon-trainable params: 0 (0.00 Byte)\n________________________________________________________________________________\n\n\n\n\n\n\nepochs <- 10 #Durchläufe\nbatch_size <- 1024\n\n# fit the model\ntic()\nhistory <- model_keras %>%\n  fit(\n    data_train_x,\n    data_train_y,\n    batch_size = batch_size,\n    epochs = epochs,\n    verbose = 1,\n    validation_data = list(data_val_x, data_val_y))\n\nEpoch 1/10\n\n1/4 [======>.......................] - ETA: 16s - loss: 0.6935 - accuracy: 0.4658\n2/4 [==============>...............] - ETA: 1s - loss: 0.6747 - accuracy: 0.5962 \n3/4 [=====================>........] - ETA: 0s - loss: 0.6550 - accuracy: 0.6484\n4/4 [==============================] - ETA: 0s - loss: 0.6543 - accuracy: 0.6595\n4/4 [==============================] - 8s 833ms/step - loss: 0.6543 - accuracy: 0.6595 - val_loss: 0.5791 - val_accuracy: 0.7440\nEpoch 2/10\n\n1/4 [======>.......................] - ETA: 2s - loss: 0.5681 - accuracy: 0.7559\n2/4 [==============>...............] - ETA: 1s - loss: 0.5920 - accuracy: 0.7373\n3/4 [=====================>........] - ETA: 0s - loss: 0.5875 - accuracy: 0.7419\n4/4 [==============================] - ETA: 0s - loss: 0.5836 - accuracy: 0.7445\n4/4 [==============================] - 3s 682ms/step - loss: 0.5836 - accuracy: 0.7445 - val_loss: 0.5684 - val_accuracy: 0.7440\nEpoch 3/10\n\n1/4 [======>.......................] - ETA: 2s - loss: 0.5577 - accuracy: 0.7539\n2/4 [==============>...............] - ETA: 1s - loss: 0.5578 - accuracy: 0.7549\n3/4 [=====================>........] - ETA: 0s - loss: 0.5733 - accuracy: 0.7435\n4/4 [==============================] - ETA: 0s - loss: 0.5720 - accuracy: 0.7445\n4/4 [==============================] - 3s 675ms/step - loss: 0.5720 - accuracy: 0.7445 - val_loss: 0.5688 - val_accuracy: 0.7440\nEpoch 4/10\n\n1/4 [======>.......................] - ETA: 2s - loss: 0.5885 - accuracy: 0.7256\n2/4 [==============>...............] - ETA: 1s - loss: 0.5644 - accuracy: 0.7485\n3/4 [=====================>........] - ETA: 0s - loss: 0.5679 - accuracy: 0.7445\n4/4 [==============================] - ETA: 0s - loss: 0.5675 - accuracy: 0.7445\n4/4 [==============================] - 3s 674ms/step - loss: 0.5675 - accuracy: 0.7445 - val_loss: 0.5665 - val_accuracy: 0.7440\nEpoch 5/10\n\n1/4 [======>.......................] - ETA: 2s - loss: 0.5616 - accuracy: 0.7500\n2/4 [==============>...............] - ETA: 1s - loss: 0.5646 - accuracy: 0.7461\n3/4 [=====================>........] - ETA: 0s - loss: 0.5628 - accuracy: 0.7467\n4/4 [==============================] - ETA: 0s - loss: 0.5649 - accuracy: 0.7445\n4/4 [==============================] - 3s 685ms/step - loss: 0.5649 - accuracy: 0.7445 - val_loss: 0.5580 - val_accuracy: 0.7440\nEpoch 6/10\n\n1/4 [======>.......................] - ETA: 2s - loss: 0.5513 - accuracy: 0.7520\n2/4 [==============>...............] - ETA: 1s - loss: 0.5411 - accuracy: 0.7588\n3/4 [=====================>........] - ETA: 0s - loss: 0.5502 - accuracy: 0.7490\n4/4 [==============================] - ETA: 0s - loss: 0.5545 - accuracy: 0.7445\n4/4 [==============================] - 3s 668ms/step - loss: 0.5545 - accuracy: 0.7445 - val_loss: 0.5425 - val_accuracy: 0.7440\nEpoch 7/10\n\n1/4 [======>.......................] - ETA: 2s - loss: 0.5356 - accuracy: 0.7490\n2/4 [==============>...............] - ETA: 1s - loss: 0.5434 - accuracy: 0.7388\n3/4 [=====================>........] - ETA: 0s - loss: 0.5335 - accuracy: 0.7454\n4/4 [==============================] - ETA: 0s - loss: 0.5325 - accuracy: 0.7445\n4/4 [==============================] - 3s 673ms/step - loss: 0.5325 - accuracy: 0.7445 - val_loss: 0.5053 - val_accuracy: 0.7440\nEpoch 8/10\n\n1/4 [======>.......................] - ETA: 2s - loss: 0.4946 - accuracy: 0.7480\n2/4 [==============>...............] - ETA: 1s - loss: 0.4917 - accuracy: 0.7397\n3/4 [=====================>........] - ETA: 0s - loss: 0.4801 - accuracy: 0.7435\n4/4 [==============================] - ETA: 0s - loss: 0.4754 - accuracy: 0.7445\n4/4 [==============================] - 3s 667ms/step - loss: 0.4754 - accuracy: 0.7445 - val_loss: 0.4414 - val_accuracy: 0.7464\nEpoch 9/10\n\n1/4 [======>.......................] - ETA: 2s - loss: 0.4158 - accuracy: 0.7480\n2/4 [==============>...............] - ETA: 1s - loss: 0.3956 - accuracy: 0.7812\n3/4 [=====================>........] - ETA: 0s - loss: 0.3909 - accuracy: 0.7965\n4/4 [==============================] - ETA: 0s - loss: 0.3891 - accuracy: 0.8029\n4/4 [==============================] - 3s 665ms/step - loss: 0.3891 - accuracy: 0.8029 - val_loss: 0.3747 - val_accuracy: 0.8667\nEpoch 10/10\n\n1/4 [======>.......................] - ETA: 2s - loss: 0.3502 - accuracy: 0.8760\n2/4 [==============>...............] - ETA: 1s - loss: 0.3474 - accuracy: 0.8799\n3/4 [=====================>........] - ETA: 0s - loss: 0.3331 - accuracy: 0.8844\n4/4 [==============================] - ETA: 0s - loss: 0.3344 - accuracy: 0.8831\n4/4 [==============================] - 3s 672ms/step - loss: 0.3344 - accuracy: 0.8831 - val_loss: 0.3434 - val_accuracy: 0.8714\n\n# history plot\nplot(history)\n\n\n\ntoc()\n\n33.88 sec elapsed\n\n\n\n\n\nDie Anwendung des Models auf die Datensätze train, validation und test ergibt nachstehende Metriken.\n\ntrain-DatensatzValidation-Datensatztest-Datensatz\n\n\n\n\nShow the Code\n# vorhersagen auf train-Datensatz\ndata_train_pred <- model_keras %>%\n  predict(data_train_x) %>%\n  k_argmax() %>%\n  as.vector()\n\n\n105/105 - 2s - 2s/epoch - 16ms/step\n\n\nShow the Code\n#Metriken train-Datensatz\nconf.matrix <-  confusionMatrix(\n  factor(data_train_pred, labels = c(\"no\", \"yes\")),\n  factor(data_train$class, labels = c(\"no\", \"yes\")),\n  positive = \"yes\"\n)\n\n#Anzeigen\nconf.matrix$byClass %>% round(digits = 3) * 100\n\n\n         Sensitivity          Specificity       Pos Pred Value \n                94.1                 85.1                 94.8 \n      Neg Pred Value            Precision               Recall \n                83.2                 94.8                 94.1 \n                  F1           Prevalence       Detection Rate \n                94.5                 74.4                 70.1 \nDetection Prevalence    Balanced Accuracy \n                73.9                 89.6 \n\n\n\n\n\n\nShow the Code\n# vorhersagen auf data_val (= Datensatz für Kreuzvalidierung)\ndata_val_pred <- model_keras %>%\n  predict(data_val_x) %>%\n  k_argmax() %>%\n  as.vector()\n\n\n27/27 - 0s - 332ms/epoch - 12ms/step\n\n\nShow the Code\n#Metriken für Teil von train-Datensatz (data_val)\nconf.matrix <-  confusionMatrix(\n  factor(data_val_pred, labels = c(\"no\", \"yes\")),\n  factor(data_val$class, labels = c(\"no\", \"yes\")),\n  positive = \"yes\"\n)\n\n#Anzeigen\nconf.matrix$byClass %>% round(digits = 3) * 100\n\n\n         Sensitivity          Specificity       Pos Pred Value \n                91.4                 74.9                 91.4 \n      Neg Pred Value            Precision               Recall \n                74.9                 91.4                 91.4 \n                  F1           Prevalence       Detection Rate \n                91.4                 74.4                 68.0 \nDetection Prevalence    Balanced Accuracy \n                74.4                 83.1 \n\n\n\n\n\n\nShow the Code\n# vorhersagen auf test-Datensatz\ndata_test_pred <- model_keras %>%\n  predict(data_test_x) %>%\n  k_argmax() %>%\n  as.vector()\n\n\n44/44 - 1s - 553ms/epoch - 13ms/step\n\n\nShow the Code\n#Metriken test-Datensatz\nconf.matrix <-  confusionMatrix(\n  factor(data_test_pred, labels = c(\"no\", \"yes\")),\n  factor(test_hate$class, labels = c(\"no\", \"yes\")),\n  positive = \"yes\"\n)\n\n#Anzeigen\nconf.matrix$byClass %>% round(digits = 3) * 100\n\n\n         Sensitivity          Specificity       Pos Pred Value \n                92.8                 79.3                 92.9 \n      Neg Pred Value            Precision               Recall \n                79.1                 92.9                 92.8 \n                  F1           Prevalence       Detection Rate \n                92.8                 74.4                 69.0 \nDetection Prevalence    Balanced Accuracy \n                74.3                 86.1 \n\n\n\n\n\nDie Hate Speech Kalassifikation fällt schlechter aus als beim Hugging Face Modell. Dennoch kann man zufrieden sein.\n\n\n\n\nNun werden Modelle mit Tidymodels erstellt. Verwendet werden hierfür Random Forest, Nearest Neighbor und XGBoost.\n\nmod_rand <- rand_forest(mode = \"classification\")\nmod_neighbor <- nearest_neighbor(mode = \"classification\")\nmod_xgb <- boost_tree(mode = \"classification\")\n\n## Mit Tuning\nmod_randT <-\n  rand_forest(mtry = tune(),\n              min_n = tune(),\n              trees = 1000, #guter Wert (keine Veränderungen durch tunen)\n              mode = \"classification\") %>% \n  set_engine(\"ranger\", num.threads = 4)\n\nmod_neighborT <- \n  nearest_neighbor(neighbors = tune(),\n                   mode = \"classification\")\n\nmod_xgbT <- \n   boost_tree(min_n = tune(), \n              trees = 1000,\n              learn_rate = tune()) %>% \n  set_mode(\"classification\") %>% \n  set_engine(\"xgboost\", nthreads = parallel::detectCores()-1)\n\n\n#Daten (ohne Tokens) und Lexikon laden\ntrain_mod <- train_hate %>% \n  select(-tweet)\ntest_mod <- test_hate %>% \n  select(-tweet)\n\nInsults <- read_excel(insult_pfad)\nnames(Insults) <- \"word\" #get_sentiment braucht Bezeichnung word\nInsults$value <- 1\n\n#recipe\nrec <- recipe(class ~ ., data = train_mod) %>%  \n  update_role(id, new_role = \"id\") %>% \n  update_role(tweet_clean, new_role = \"ignore\") %>% #Spalte wird kein Prädiktor\n  step_mutate(insult = get_sentiment(tweet_clean,  \n                                    method = \"custom\",\n                                    lexicon = Insults)) %>% \n  step_textfeature(tweet_clean, keep_original_cols = TRUE) %>% \n  step_nzv(all_predictors()) %>% #textfeature erzeugt viele sinnlose Spalten\n  step_tokenize(tweet_clean) %>%\n  step_stopwords(tweet_clean, keep = FALSE) %>%\n  step_tokenfilter(tweet_clean, max_tokens = 1e3) %>% \n  step_tfidf(tweet_clean, keep_original_cols = TRUE) %>% \n  step_nzv(all_predictors()) %>%  #(zweimal, um sofort Spalten zu entfernen)\n  step_normalize(all_numeric_predictors())\n\n\nrec_prepped <- prep(rec)\ndata_prep <- bake(rec_prepped, new_data = NULL)\n# Erkenntisse bake: tfidf wird durch step_nzv komplett entfernt\n\n\n#Kreuzvalidierung\nset.seed(42)\nkv <- vfold_cv(train_mod, strata = class)\n\nDie Leistung der trainierten Modelle fällt wie folgt aus.\n\n\n\nRandom ForestNearest NeighborXGBoost\n\n\n\n\nShow the Code\nwf_rand <-\n  workflow() %>% \n  add_model(mod_rand) %>% \n  add_recipe(rec)\n\ntic()\nfit_rand <-\n  fit(wf_rand,\n      data = train_mod)\ntoc()\n\n\n9.92 sec elapsed\n\n\nShow the Code\n#Vorhersage\ntic()\npreds <-\n  predict(fit_rand, new_data = test_mod)\ntoc()\n\n\n1.77 sec elapsed\n\n\nShow the Code\n#Test\nVorgersage <-\n  test_mod %>%  \n  bind_cols(preds)\n\nmy_metrics <- metric_set(accuracy, f_meas)\nmy_metrics(Vorgersage,\n           truth = as.factor(class), #estimate ist factor, also class umwandeln\n           estimate = .pred_class)\n\n\n\n\n  \n\n\n\n\n\n\n\nShow the Code\nwf_n <-\n  workflow() %>% \n  add_model(mod_neighbor) %>% \n  add_recipe(rec)\n\ntic()\nfit_n <-\n  fit(wf_n,\n      data = train_mod)\ntoc()\n\n\n8.54 sec elapsed\n\n\nShow the Code\n#Vorhersage\ntic()\npreds <-\n  predict(fit_n, new_data = test_mod)\ntoc()\n\n\n1.49 sec elapsed\n\n\nShow the Code\n#Test\nVorgersage <-\n  test_mod %>%  \n  bind_cols(preds)\n\nmy_metrics <- metric_set(accuracy, f_meas)\nmy_metrics(Vorgersage,\n           truth = as.factor(class), #estimate ist factor, also class umwandeln\n           estimate = .pred_class)\n\n\n\n\n  \n\n\n\n\n\n\n\nShow the Code\nwf_boost <-\n  workflow() %>% \n  add_model(mod_xgb) %>% \n  add_recipe(rec)\n\ntic()\nfit_boost <-\n  fit(wf_boost,\n      data = train_mod)\ntoc()\n\n\n8.25 sec elapsed\n\n\nShow the Code\n#Vorhersage\ntic()\npreds <-\n  predict(fit_boost, new_data = test_mod)\ntoc()\n\n\n1.36 sec elapsed\n\n\nShow the Code\n#Test\nVorgersage <-\n  test_mod %>%  \n  bind_cols(preds)\n\nmy_metrics <- metric_set(accuracy, f_meas)\nmy_metrics(Vorgersage,\n           truth = as.factor(class), #estimate ist factor, also class umwandeln\n           estimate = .pred_class)\n\n\n\n\n  \n\n\n\n\n\n\nOhne Tuning erhält man bereits gute Accuracy-Werte, welche gut mit dem LSTM Modell mithalten können. Die Rechenzeit von unter 10 Sekunden ist ebenfalls sehr angenehm.\nMal sehen, ob sich die Modelle durch Tuning verbessern.\n\n\n\n\nRandom ForestNearest NeighborXGBoost\n\n\n\n\nShow the Code\nwf_randT <-\n  workflow() %>% \n  add_model(mod_randT) %>% \n  add_recipe(rec)\n\ntic()\nset.seed(42)\nrf_tune <- tune_grid(object = wf_randT, resamples = kv, grid = 5,\n                         control = control_grid(save_workflow = TRUE))\ntoc()\n\n\n151.19 sec elapsed\n\n\nShow the Code\n#Ergebnisse\nbest_model_rf <- fit_best(rf_tune)\n\nrf_aufTest <- last_fit(best_model_rf, train_test_split)\nrf_aufTest %>% collect_metrics()\n\n\n\n\n  \n\n\n\n\n\n\n\nShow the Code\nwf_knnT <-\n  workflow() %>% \n  add_model(mod_neighborT) %>% \n  add_recipe(rec)\n\ntic()\nset.seed(42)\nknn_tune <- tune_grid(object = wf_knnT, resamples = kv, grid = 5,\n                         control = control_grid(save_workflow = TRUE))\ntoc()\n\n\n85.98 sec elapsed\n\n\nShow the Code\n#Ergebnisse\nbest_model_knn <- fit_best(knn_tune)\n\nknn_aufTest <- last_fit(best_model_knn, train_test_split)\nknn_aufTest %>% collect_metrics()\n\n\n\n\n  \n\n\n\n\n\n\n\nShow the Code\nwf_xgbT <-\n  workflow() %>% \n  add_model(mod_xgbT) %>% \n  add_recipe(rec)\n\ntic()\nset.seed(42)\nxgb_tune <- tune_grid(object = wf_xgbT, resamples = kv, grid = 5,\n                         control = control_grid(save_workflow = TRUE))\ntoc()\n\n\n204.5 sec elapsed\n\n\nShow the Code\n#Ergebnisse\nbest_model_xgb <- fit_best(xgb_tune)\n\n\n[22:07:42] WARNING: src/learner.cc:767: \nParameters: { \"nthreads\" } are not used.\n\n\nShow the Code\nxgb_aufTest <- last_fit(best_model_xgb, train_test_split)\nxgb_aufTest %>% collect_metrics()\n\n\n\n\n  \n\n\n\n\n\n\nWenn man die Accuracy zwischen den optimierten und den nicht optimierten Modellen vergleicht, ist diese kaum besser oder gleich gut. Folglich kann man sich in diesem Fall den erhöhten Zeitaufwand, der durch das Tuning entsteht, sparen.\n\n\n\n\nDie Klassifikation von Hate Speech in Textnachrichten stellt eine komplexe und anspruchsvolle Aufgabe dar. Durch den Einsatz von maschinellen Lernmodellen und Textanalysemethoden konnten jedoch vielversprechende Ergebnisse erzielt werden.\nInsgesamt kann festgestellt werden, dass das Hugging Face Modell am besten abschnitt. Dennoch konnten die selbst trainierten Modelle gut mithalten. Die nicht optimierten Tidymodels Modelle sind mit einer Rechenzeit von ca. 10 Sekunden extrem schnell. Wenn man schnelle Ergebnisse möchte, sind diese auf jeden Fall zu Empfehlen.\nDer Post zeigt, dass der Einsatz von Machine-Learning-Techniken die Bekämpfung von Hassrede in den sozialen Medien unterstützen kann. Es bleibt jedoch eine Herausforderung, diese Technologie verantwortungsvoll einzusetzen und sicherzustellen, dass die Meinungsfreiheit respektiert wird und keine unbeabsichtigten Nebenwirkungen entstehen."
  },
  {
    "objectID": "VirtualEnv/Lib/site-packages/huggingface_hub/templates/datasetcard_template.html",
    "href": "VirtualEnv/Lib/site-packages/huggingface_hub/templates/datasetcard_template.html",
    "title": "blognils",
    "section": "",
    "text": "{{ dataset_summary | default(““, true) }}\n\n\n\n\n\n{{ dataset_description | default(““, true) }}\n\nCurated by: {{ curators | default(“[More Information Needed]”, true)}}\nFunded by [optional]: {{ funded_by | default(“[More Information Needed]”, true)}}\nShared by [optional]: {{ shared_by | default(“[More Information Needed]”, true)}}\nLanguage(s) (NLP): {{ language | default(“[More Information Needed]”, true)}}\nLicense: {{ license | default(“[More Information Needed]”, true)}}\n\n\n\n\n\n\nRepository: {{ repo | default(“[More Information Needed]”, true)}}\nPaper [optional]: {{ paper | default(“[More Information Needed]”, true)}}\nDemo [optional]: {{ demo | default(“[More Information Needed]”, true)}}\n\n\n\n\n\n\n\n\n\n\n{{ direct_use | default(“[More Information Needed]”, true)}}\n\n\n\n\n{{ out_of_scope_use | default(“[More Information Needed]”, true)}}\n\n\n\n\n\n{{ dataset_structure | default(“[More Information Needed]”, true)}}\n\n\n\n\n\n\n{{ curation_rationale_section | default(“[More Information Needed]”, true)}}\n\n\n\n\n\n\n\n{{ data_collection_and_processing_section | default(“[More Information Needed]”, true)}}\n\n\n\n\n{{ source_data_producers_section | default(“[More Information Needed]”, true)}}\n\n\n\n\n\n\n\n\n{{ annotation_process_section | default(“[More Information Needed]”, true)}}\n\n\n\n\n{{ who_are_annotators_section | default(“[More Information Needed]”, true)}}\n\n\n\n\n{{ personal_and_sensitive_information | default(“[More Information Needed]”, true)}}\n\n\n\n\n\n\n{{ bias_risks_limitations | default(“[More Information Needed]”, true)}}\n\n\n\n{{ bias_recommendations | default(“Users should be made aware of the risks, biases and limitations of the dataset. More information needed for further recommendations.”, true)}}\n\n\n\n\n\nBibTeX:\n{{ citation_bibtex | default(“[More Information Needed]”, true)}}\nAPA:\n{{ citation_apa | default(“[More Information Needed]”, true)}}\n\n\n\n\n{{ glossary | default(“[More Information Needed]”, true)}}\n\n\n\n{{ more_information | default(“[More Information Needed]”, true)}}\n\n\n\n{{ dataset_card_authors | default(“[More Information Needed]”, true)}}\n\n\n\n{{ dataset_card_contact | default(“[More Information Needed]”, true)}}"
  },
  {
    "objectID": "VirtualEnv/Lib/site-packages/huggingface_hub/templates/modelcard_template.html",
    "href": "VirtualEnv/Lib/site-packages/huggingface_hub/templates/modelcard_template.html",
    "title": "blognils",
    "section": "",
    "text": "{{ model_summary | default(““, true) }}\n\n\n\n\n\n{{ model_description | default(““, true) }}\n\nDeveloped by: {{ developers | default(“[More Information Needed]”, true)}}\nFunded by [optional]: {{ funded_by | default(“[More Information Needed]”, true)}}\nShared by [optional]: {{ shared_by | default(“[More Information Needed]”, true)}}\nModel type: {{ model_type | default(“[More Information Needed]”, true)}}\nLanguage(s) (NLP): {{ language | default(“[More Information Needed]”, true)}}\nLicense: {{ license | default(“[More Information Needed]”, true)}}\nFinetuned from model [optional]: {{ base_model | default(“[More Information Needed]”, true)}}\n\n\n\n\n\n\nRepository: {{ repo | default(“[More Information Needed]”, true)}}\nPaper [optional]: {{ paper | default(“[More Information Needed]”, true)}}\nDemo [optional]: {{ demo | default(“[More Information Needed]”, true)}}\n\n\n\n\n\n\n\n\n\n{{ direct_use | default(“[More Information Needed]”, true)}}\n\n\n\n\n{{ downstream_use | default(“[More Information Needed]”, true)}}\n\n\n\n\n{{ out_of_scope_use | default(“[More Information Needed]”, true)}}\n\n\n\n\n\n{{ bias_risks_limitations | default(“[More Information Needed]”, true)}}\n\n\n\n{{ bias_recommendations | default(“Users (both direct and downstream) should be made aware of the risks, biases and limitations of the model. More information needed for further recommendations.”, true)}}\n\n\n\n\nUse the code below to get started with the model.\n{{ get_started_code | default(“[More Information Needed]”, true)}}\n\n\n\n\n\n\n{{ training_data | default(“[More Information Needed]”, true)}}\n\n\n\n\n\n\n{{ preprocessing | default(“[More Information Needed]”, true)}}\n\n\n\n\nTraining regime: {{ training_regime | default(“[More Information Needed]”, true)}} \n\n\n\n\n\n{{ speeds_sizes_times | default(“[More Information Needed]”, true)}}\n\n\n\n\n\n\n\n\n\n\n\n{{ testing_data | default(“[More Information Needed]”, true)}}\n\n\n\n\n{{ testing_factors | default(“[More Information Needed]”, true)}}\n\n\n\n\n{{ testing_metrics | default(“[More Information Needed]”, true)}}\n\n\n\n\n{{ results | default(“[More Information Needed]”, true)}}\n\n\n{{ results_summary | default(““, true) }}\n\n\n\n\n\n\n{{ model_examination | default(“[More Information Needed]”, true)}}\n\n\n\n\nCarbon emissions can be estimated using the Machine Learning Impact calculator presented in Lacoste et al. (2019).\n\nHardware Type: {{ hardware_type | default(“[More Information Needed]”, true)}}\nHours used: {{ hours_used | default(“[More Information Needed]”, true)}}\nCloud Provider: {{ cloud_provider | default(“[More Information Needed]”, true)}}\nCompute Region: {{ cloud_region | default(“[More Information Needed]”, true)}}\nCarbon Emitted: {{ co2_emitted | default(“[More Information Needed]”, true)}}\n\n\n\n\n\n\n{{ model_specs | default(“[More Information Needed]”, true)}}\n\n\n\n{{ compute_infrastructure | default(“[More Information Needed]”, true)}}\n\n\n{{ hardware_requirements | default(“[More Information Needed]”, true)}}\n\n\n\n{{ software | default(“[More Information Needed]”, true)}}\n\n\n\n\n\n\nBibTeX:\n{{ citation_bibtex | default(“[More Information Needed]”, true)}}\nAPA:\n{{ citation_apa | default(“[More Information Needed]”, true)}}\n\n\n\n\n{{ glossary | default(“[More Information Needed]”, true)}}\n\n\n\n{{ more_information | default(“[More Information Needed]”, true)}}\n\n\n\n{{ model_card_authors | default(“[More Information Needed]”, true)}}\n\n\n\n{{ model_card_contact | default(“[More Information Needed]”, true)}}"
  },
  {
    "objectID": "VirtualEnv/Lib/site-packages/idna-3.6.dist-info/LICENSE.html",
    "href": "VirtualEnv/Lib/site-packages/idna-3.6.dist-info/LICENSE.html",
    "title": "blognils",
    "section": "",
    "text": "BSD 3-Clause License\nCopyright (c) 2013-2023, Kim Davies and contributors. All rights reserved.\nRedistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met:\n\nRedistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer.\nRedistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution.\nNeither the name of the copyright holder nor the names of its contributors may be used to endorse or promote products derived from this software without specific prior written permission.\n\nTHIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS “AS IS” AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE."
  },
  {
    "objectID": "VirtualEnv/Lib/site-packages/Markdown-3.5.2.dist-info/LICENSE.html",
    "href": "VirtualEnv/Lib/site-packages/Markdown-3.5.2.dist-info/LICENSE.html",
    "title": "blognils",
    "section": "",
    "text": "Copyright 2007, 2008 The Python Markdown Project (v. 1.7 and later) Copyright 2004, 2005, 2006 Yuri Takhteyev (v. 0.2-1.6b) Copyright 2004 Manfred Stienstra (the original version)\nAll rights reserved.\nRedistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met:\n\nRedistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer.\nRedistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution.\nNeither the name of the Python Markdown Project nor the names of its contributors may be used to endorse or promote products derived from this software without specific prior written permission.\n\nTHIS SOFTWARE IS PROVIDED BY THE PYTHON MARKDOWN PROJECT ‘’AS IS’’ AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL ANY CONTRIBUTORS TO THE PYTHON MARKDOWN PROJECT BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE."
  },
  {
    "objectID": "VirtualEnv/Lib/site-packages/numpy/random/LICENSE.html",
    "href": "VirtualEnv/Lib/site-packages/numpy/random/LICENSE.html",
    "title": "blognils",
    "section": "",
    "text": "This software is dual-licensed under the The University of Illinois/NCSA Open Source License (NCSA) and The 3-Clause BSD License\n\nNCSA Open Source License\nCopyright (c) 2019 Kevin Sheppard. All rights reserved.\nDeveloped by: Kevin Sheppard (kevin.sheppard@economics.ox.ac.uk, kevin.k.sheppard@gmail.com) http://www.kevinsheppard.com\nPermission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the “Software”), to deal with the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:\nRedistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimers.\nRedistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimers in the documentation and/or other materials provided with the distribution.\nNeither the names of Kevin Sheppard, nor the names of any contributors may be used to endorse or promote products derived from this Software without specific prior written permission.\nTHE SOFTWARE IS PROVIDED “AS IS”, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE CONTRIBUTORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS WITH THE SOFTWARE.\n\n\n3-Clause BSD License\nCopyright (c) 2019 Kevin Sheppard. All rights reserved.\nRedistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met:\n\nRedistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer.\nRedistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution.\nNeither the name of the copyright holder nor the names of its contributors may be used to endorse or promote products derived from this software without specific prior written permission.\n\nTHIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS “AS IS” AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n\n\nComponents\nMany parts of this module have been derived from original sources, often the algorithm’s designer. Component licenses are located with the component code."
  },
  {
    "objectID": "VirtualEnv/Lib/site-packages/tensorflow/include/external/libjpeg_turbo/LICENSE.html",
    "href": "VirtualEnv/Lib/site-packages/tensorflow/include/external/libjpeg_turbo/LICENSE.html",
    "title": "blognils",
    "section": "",
    "text": "libjpeg-turbo Licenses\nlibjpeg-turbo is covered by three compatible BSD-style open source licenses:\n\nThe IJG (Independent JPEG Group) License, which is listed in README.ijg\nThis license applies to the libjpeg API library and associated programs (any code inherited from libjpeg, and any modifications to that code.)\nThe Modified (3-clause) BSD License, which is listed below\nThis license covers the TurboJPEG API library and associated programs, as well as the build system.\nThe zlib License\nThis license is a subset of the other two, and it covers the libjpeg-turbo SIMD extensions.\n\n\n\nComplying with the libjpeg-turbo Licenses\nThis section provides a roll-up of the libjpeg-turbo licensing terms, to the best of our understanding.\n\nIf you are distributing a modified version of the libjpeg-turbo source, then:\n\nYou cannot alter or remove any existing copyright or license notices from the source.\nOrigin\n\nClause 1 of the IJG License\nClause 1 of the Modified BSD License\nClauses 1 and 3 of the zlib License\n\nYou must add your own copyright notice to the header of each source file you modified, so others can tell that you modified that file (if there is not an existing copyright header in that file, then you can simply add a notice stating that you modified the file.)\nOrigin\n\nClause 1 of the IJG License\nClause 2 of the zlib License\n\nYou must include the IJG README file, and you must not alter any of the copyright or license text in that file.\nOrigin\n\nClause 1 of the IJG License\n\n\nIf you are distributing only libjpeg-turbo binaries without the source, or if you are distributing an application that statically links with libjpeg-turbo, then:\n\nYour product documentation must include a message stating:\nThis software is based in part on the work of the Independent JPEG Group.\nOrigin\n\nClause 2 of the IJG license\n\nIf your binary distribution includes or uses the TurboJPEG API, then your product documentation must include the text of the Modified BSD License (see below.)\nOrigin\n\nClause 2 of the Modified BSD License\n\n\nYou cannot use the name of the IJG or The libjpeg-turbo Project or the contributors thereof in advertising, publicity, etc.\nOrigin\n\nIJG License\nClause 3 of the Modified BSD License\n\nThe IJG and The libjpeg-turbo Project do not warrant libjpeg-turbo to be free of defects, nor do we accept any liability for undesirable consequences resulting from your use of the software.\nOrigin\n\nIJG License\nModified BSD License\nzlib License\n\n\n\n\nThe Modified (3-clause) BSD License\nCopyright (C)2009-2022 D. R. Commander. All Rights Reserved. Copyright (C)2015 Viktor Szathmáry. All Rights Reserved.\nRedistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met:\n\nRedistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer.\nRedistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution.\nNeither the name of the libjpeg-turbo Project nor the names of its contributors may be used to endorse or promote products derived from this software without specific prior written permission.\n\nTHIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS “AS IS”, AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDERS OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n\n\nWhy Three Licenses?\nThe zlib License could have been used instead of the Modified (3-clause) BSD License, and since the IJG License effectively subsumes the distribution conditions of the zlib License, this would have effectively placed libjpeg-turbo binary distributions under the IJG License. However, the IJG License specifically refers to the Independent JPEG Group and does not extend attribution and endorsement protections to other entities. Thus, it was desirable to choose a license that granted us the same protections for new code that were granted to the IJG for code derived from their software."
  },
  {
    "objectID": "VirtualEnv/Lib/site-packages/werkzeug/debug/shared/ICON_LICENSE.html",
    "href": "VirtualEnv/Lib/site-packages/werkzeug/debug/shared/ICON_LICENSE.html",
    "title": "blognils",
    "section": "",
    "text": "Silk icon set 1.3 by Mark James mjames@gmail.com\nhttp://www.famfamfam.com/lab/icons/silk/\nLicense: CC-BY-2.5 or CC-BY-3.0"
  },
  {
    "objectID": "RenderPRO.html",
    "href": "RenderPRO.html",
    "title": "Hate Speech Klassifikation",
    "section": "",
    "text": "Die Klassifikation von Hassrede in Twitternachrichten ist entscheidend, um die Auswirkungen von negativem Verhalten in sozialen Medien zu verstehen und einzudämmen. Durch die Entwicklung von Algorithmen und Modellen, die automatisch Hassrede erkennen, können wir potenziell schädliche Inhalte identifizieren und bekämpfen. Dieser Post bietet einen Überblick über Ansätze und Techniken zur Hassredeerkennung.\n\n\n\nlibrary(tidyverse)\nlibrary(tidytext)\nlibrary(tidymodels)\nlibrary(wordcloud)\nlibrary(tictoc)\nlibrary(caret) #für Konfusionsmatrix\n\n\n\n\n\nd_hate <- read_csv(pfad)\n\n\n\n\n## Tweets bereinigen\n\nclean.text = function(x)\n{\n  # alles in Kleinbuchstaben umwandeln\n  x = tolower(x)\n  # rt entfernen\n  x = gsub(\"rt\", \"\", x)\n  # alle @ + Namen entfernen\n  x = gsub(\"@\\\\w+\", \"\", x)\n  # Satzzeichen entfernen\n  x = gsub(\"[[:punct:]]\", \"\", x)\n  # Zahlen entfernen\n  x = gsub(\"[[:digit:]]\", \"\", x)\n  # Links entfernen\n  x = gsub(\"http\\\\w+\", \"\", x)\n  # hhtp entfernen wenn es alleine steht\n  x = gsub(\"http\", \"\", x)\n  # doppelte Leerzeichen und Tabs entfernen\n  x = gsub(\"[ |\\t]{2,}\", \" \", x)\n  # Leerzeichen am Anfang entfernen\n  x = gsub(\"^ \", \"\", x)\n  # Leerzeichen am Ende entfernen\n  x = gsub(\" $\", \"\", x)\n\n  return(x)\n}\n\ndf_hate <- d_hate %>% \n  mutate(tweet_clean = tweet) %>% \n  mutate(tweet_clean = clean.text(tweet_clean))\n\n#Splitten\nset.seed(142)\ntrain_test_split <- initial_split(df_hate, strata = class)\ntrain_hate <- training(train_test_split)\ntest_hate <- testing(train_test_split)\n\n\n\n\n\ndf_hate <- df_hate %>%\n  mutate(words = tweet_clean) %>% \n  unnest_tokens(words, words)\n\n## die 15 häufigsten Wörter\n\ndf_hate %>%\n  count(words, sort = TRUE) %>%\n  top_n(15) %>%\n  mutate(words = reorder(words, n)) %>%\n  ggplot(aes(x = words, y = n)) +\n  geom_col() +\n  xlab(NULL) +\n  coord_flip() +\n      labs(x = \"Wort\",\n      y = \"Anzahl\",\n      title = \"Die 15 häufigsten Wörter\") +\n  theme_minimal()\n\nSelecting by n\n\n\n\n\n\nDiese Graphik ist eine schöne Demonstration, warum Stoppwörter entfernt werden sollten. Wörter wie “the”, “a” oder “and” liefern keine Hinweise auf die Stimmung im Tweet.\n\ndata(\"stop_words\")\n\n# Stoppwortentfernung\n\ndf_hate <- df_hate %>%\n  anti_join(stop_words, by = c(\"words\" = \"word\"))\n\n\n## die 15 häufigsten Wörter\n\ndf_hate %>%\n  count(words, sort = TRUE) %>%\n  top_n(15) %>%\n  mutate(words = reorder(words, n)) %>%\n  ggplot(aes(x = words, y = n)) +\n  geom_col() +\n  xlab(NULL) +\n  coord_flip() +\n      labs(x = \"Wort\",\n      y = \"Anzahl\",\n      title = \"Die 15 häufigsten Wörter\") +\n  theme_minimal()\n\nSelecting by n\n\n\n\n\n\n\n\n\n\nUm sich die Tweets genauer vor Augen zu führen, werden diese im folgenden graphisch dargestellt. ### Stimmungsanalyse\n\nlibrary(syuzhet)\n\n\nAttache Paket: 'syuzhet'\n\n\nDas folgende Objekt ist maskiert 'package:scales':\n\n    rescale\n\n# Converting tweets to ASCII to trackle strange characters\ntweets <- df_hate %>% \n  select(tweet_clean)\n\ntweets <- iconv(tweets, from=\"UTF-8\", to=\"ASCII\", sub=\"\")\n\n#Sentiments erkennen\nsentiments <- get_nrc_sentiment((tweets))\n\n#Score erstellen\nsentimentscores<-data.frame(colSums(sentiments[,]))\n\n#Spalte umbenennen\nnames(sentimentscores) <- \"Score\"\n\n#neue Spalte \"sentiment\"\nsentimentscores <- cbind(\"sentiment\"=rownames(sentimentscores),sentimentscores)\nrownames(sentimentscores) <- NULL\n\n#plotten\nggplot(data=sentimentscores,aes(x=sentiment,y=Score))+\n  geom_bar(aes(fill=sentiment),stat = \"identity\")+\n  theme(legend.position=\"none\")+\n  xlab(\"Sentiments\")+ylab(\"Scores\")+\n  ggtitle(\"Stimmung in den Tweets\")+\n  theme_minimal()+\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n\n\n\n\n\n\n\nlibrary(tm)\n\nLade nötiges Paket: NLP\n\n\n\nAttache Paket: 'NLP'\n\n\nDas folgende Objekt ist maskiert 'package:ggplot2':\n\n    annotate\n\n# Worttoken umwandeln\ntweets <- Corpus(VectorSource(df_hate$words))\n\nset.seed(42)\nwordcloud(tweets, min.freq = 1, max.words = 100, scale = c(2.2,1),\n          colors=brewer.pal(8, \"Dark2\"), random.color = T, random.order = F,\n          rot.per=0.25)\n\n\n\n\n\nHate SpeechNon Hate Speech\n\n\n\n\nShow the Code\n## Wortwolke Hate Speech\n\n# Worttoken umwandeln\ntweets <- df_hate %>% \n  filter(class == \"hate speech\")\n\ntweets <- Corpus(VectorSource(tweets$words))\n\n#Cloud erstellen\nset.seed(42)\nwordcloud(tweets, min.freq = 1, max.words = 100, scale = c(2.2,1),\n          colors=brewer.pal(8, \"Dark2\"), random.color = T, random.order = F,\n          rot.per=0.25)\n\n\n\n\n\n\n\n\n\nShow the Code\n## Wortwolke Non Hate Speech\n\n# Worttoken umwandeln\ntweets <- df_hate %>% \n  filter(class != \"hate speech\")\n\ntweets <- Corpus(VectorSource(tweets$words))\n\n#Cloud erstellen\nset.seed(42)\nwordcloud(tweets, min.freq = 1, max.words = 100, scale = c(2.2,1),\n          colors=brewer.pal(8, \"Dark2\"), random.color = T, random.order = F,\n          rot.per=0.25)\n\n\n\n\n\n\n\n\n\n\nShow the Code\ntweets <- df_hate %>% \n  filter(class != \"hate speech\") %>% \n  filter(words == \"hoe\")\n\ntweets[5, \"tweet_clean\"]\n\n\n\n\n  \n\n\n\n\n\n\nDie am häufigsten vorkommenden Wortpaare sind in folgender interaktiven Graphik zu sehen.\n\n## Bigram interaktiv\n\n#bigram Tabelle\nbigram <- df_hate %>% \n  unnest_tokens(\n    input = tweet_clean, \n    output = bigram, \n    token = 'ngrams', \n    n = 2\n  ) %>% \n  filter(! is.na(bigram))\n\nbigram %<>% \n  separate(col = bigram, into = c('word1', 'word2'), sep = ' ') %>% \n  filter(! word1 %in% stop_words$word) %>% \n  filter(! word2 %in% stop_words$word) %>% \n  filter(! is.na(word1)) %>% \n  filter(! is.na(word2)) \n\nbigram_count <- bigram %>% \n  count(word1, word2, sort = TRUE) %>% \n  rename(weight = n)\n\nbigram_count %>% head()\n\n\n\n  \n\n\n#bigram erstellen\n\nlibrary(networkD3)\nlibrary(igraph)\n\n\nAttache Paket: 'igraph'\n\n\nDie folgenden Objekte sind maskiert von 'package:dials':\n\n    degree, neighbors\n\n\nDie folgenden Objekte sind maskiert von 'package:lubridate':\n\n    %--%, union\n\n\nDie folgenden Objekte sind maskiert von 'package:dplyr':\n\n    as_data_frame, groups, union\n\n\nDie folgenden Objekte sind maskiert von 'package:purrr':\n\n    compose, simplify\n\n\nDas folgende Objekt ist maskiert 'package:tidyr':\n\n    crossing\n\n\nDas folgende Objekt ist maskiert 'package:tibble':\n\n    as_data_frame\n\n\nDie folgenden Objekte sind maskiert von 'package:stats':\n\n    decompose, spectrum\n\n\nDas folgende Objekt ist maskiert 'package:base':\n\n    union\n\nlibrary(magrittr)\n\n\nAttache Paket: 'magrittr'\n\n\nDas folgende Objekt ist maskiert 'package:purrr':\n\n    set_names\n\n\nDas folgende Objekt ist maskiert 'package:tidyr':\n\n    extract\n\nthreshold <- 50\n\nnetwork <-  bigram_count %>%\n  filter(weight > threshold) %>%\n  graph_from_data_frame(directed = FALSE)\n\n# Anzahl der Kanten\nV(network)$degree <- strength(graph = network)\n# Gewichtung\nE(network)$width <- E(network)$weight/max(E(network)$weight)\n\n# Create networkD3 object.\nnetwork.D3 <- igraph_to_networkD3(g = network)\n# Define node size.\nnetwork.D3$nodes %<>% mutate(Degree = (1E-2)*V(network)$degree)\n# Define color group\nnetwork.D3$nodes %<>% mutate(Group = 1)\n# Define edges width. \nnetwork.D3$links$Width <- 10*E(network)$width\n\nforceNetwork(\n  Links = network.D3$links, \n  Nodes = network.D3$nodes, \n  Source = 'source', \n  Target = 'target',\n  NodeID = 'name',\n  Group = 'Group', \n  opacity = 0.9,\n  Value = 'Width',\n  Nodesize = 'Degree', \n  # We input a JavaScript function.\n  linkWidth = JS(\"function(d) { return Math.sqrt(d.value); }\"), \n  fontSize = 12,\n  zoom = TRUE, \n  opacityNoHover = 1\n)\n\n\n\n\n\n\n\n\n\n\nlibrary(reticulate)\nuse_virtualenv(\"~/Studium/blognilsbiller/VirtualEnv\")\n\n\nfrom transformers import pipeline\n\nWARNING:tensorflow:From C:\\Users\\NILS~1.DES\\DOCUME~1\\Studium\\BLOGNI~1\\VIRTUA~1\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n\nimport tensorflow\n\n\nclassifier = pipeline(\"text-classification\", model=\"facebook/roberta-hate-speech-dynabench-r4-target\")\n\nhier kopiert\n\ntweets <- test_hate$tweet_clean\n\n\ntweets = r.tweets\nresults = classifier(tweets)\n\n\nresult <- py$results\nlabels <- lapply(result, function(element) element$label)\ntweets_hate <- cbind(test_hate, pred = unlist(labels))\ntweets_hate <- tweets_hate %>% \n  mutate(class = as.factor(class),\n         pred = case_when(pred == \"hate\" ~ \"hate speech\",\n            pred == \"nothate\" ~ \"other\"),\n         pred = as.factor(pred))\n\n\nmy_metrics2 <- metric_set(accuracy, f_meas)\nmy_metrics2(tweets_hate,\n           truth = class,\n           estimate = pred)\n\n\n\n  \n\n\n\n\n\n\n\n\n\nlibrary(keras)\n\n\nAttache Paket: 'keras'\n\n\nDie folgenden Objekte sind maskiert von 'package:igraph':\n\n    %<-%, normalize\n\n\nDas folgende Objekt ist maskiert 'package:yardstick':\n\n    get_weights\n\n# Datensatz bereinigt, ohne bereits Tokenisiert\ndata_all <- rbind(train_hate, test_hate)\n\n# Tokenisieren\nnum_words_train <- 1024\ntokens <- text_tokenizer(num_words = num_words_train,\n                            lower = TRUE) %>% \n  fit_text_tokenizer(data_all$tweet_clean)\n\n\n#train splitten für Kreuzvalidierung\nset.seed(142)\ntrain_split <- initial_split(train_hate, strata = class, prop = 0.8)\ndata_train <- training(train_split)\ndata_val <- testing(train_split)\n\n\n#maximale Länge eines Tweets\nmaxlen <- max(str_count(train_hate$tweet_clean, \"\\\\w+\")) + 1\n\n\n# prepare data for predictors x\ndata_train_x <- texts_to_sequences(tokens, data_train$tweet_clean) %>%\n  pad_sequences(maxlen = maxlen) \ndata_val_x <- texts_to_sequences(tokens, data_val$tweet_clean) %>%\n  pad_sequences(maxlen = maxlen) \ndata_test_x <- texts_to_sequences(tokens, test_hate$tweet_clean) %>%\n  pad_sequences(maxlen = maxlen)\n\n# Konvertieren der Klassenlabels in numerische Form\ndata_train <- data_train %>% \n  mutate(class = as.numeric(factor(class)))\n\ndata_val <- data_val %>% \n  mutate(class = as.numeric(factor(class)))\n\n# prepare data for target y\ndata_train_y <- to_categorical(data_train$class - 1, num_classes = 2)\ndata_val_y <- to_categorical(data_val$class - 1, num_classes = 2)\n\n#für Reproduzierbarkeit Zufallszahlengenerierung in R zu konfigurieren\nRNGkind(sample.kind = \"Rounding\")\ninitializer <- initializer_random_normal(seed = 100)\n\n## Model erstellen\nmodel_keras <- keras_model_sequential()\n\n# layer lstm 1 settings\nunit_lstm1 <- 256\ndropout_lstm1 <- 0.5\nrecurrent_dropout_lstm1 <- 0.5\n\n# layer lstm 2 settings\nunit_lstm2 <- 32\ndropout_lstm2 <- 0.5\nrecurrent_dropout_lstm2 <- 0.5\n\nmodel_keras %>%\n  layer_embedding(\n    name = \"input\",\n    input_dim = num_words_train,\n    input_length = maxlen,\n    output_dim = maxlen\n  ) %>%\n  layer_dropout(\n    name = \"embedding_dropout\",\n    rate = 0.6\n  ) %>%\n   # lstm1\n  layer_lstm(\n    name = \"lstm1\",\n    units = unit_lstm1,\n    dropout = dropout_lstm1,\n    recurrent_dropout = recurrent_dropout_lstm1,\n    return_sequences = TRUE\n  ) %>%\n  # lstm2\n  layer_lstm(\n    name = \"lstm2\",\n    units = unit_lstm2,\n    dropout = dropout_lstm2,\n    recurrent_dropout = recurrent_dropout_lstm2,\n    return_sequences = FALSE\n  ) %>%\n  # output layer\n  layer_dense(\n    name = \"output\",\n    units = 2,\n    activation = \"sigmoid\"\n  )\n\n# Compile Model\nmodel_keras %>% \n  compile(optimizer = optimizer_adam(learning_rate = 0.01),\n          metrics = \"accuracy\",\n          loss = \"binary_crossentropy\")\n\n# model summary\nsummary(model_keras)\n\nModel: \"sequential\"\n________________________________________________________________________________\n Layer (type)                       Output Shape                    Param #     \n================================================================================\n input (Embedding)                  (None, 33, 33)                  33792       \n embedding_dropout (Dropout)        (None, 33, 33)                  0           \n lstm1 (LSTM)                       (None, 33, 256)                 296960      \n lstm2 (LSTM)                       (None, 32)                      36992       \n output (Dense)                     (None, 2)                       66          \n================================================================================\nTotal params: 367810 (1.40 MB)\nTrainable params: 367810 (1.40 MB)\nNon-trainable params: 0 (0.00 Byte)\n________________________________________________________________________________\n\n\n\n\n\n\nepochs <- 10 #Durchläufe\nbatch_size <- 1024\n\nset.seed(42)\n# fit the model\ntic()\nhistory <- model_keras %>%\n  fit(\n    data_train_x,\n    data_train_y,\n    batch_size = batch_size,\n    epochs = epochs,\n    verbose = 1,\n    validation_data = list(data_val_x, data_val_y))\n\nEpoch 1/10\n\n1/4 [======>.......................] - ETA: 17s - loss: 0.6931 - accuracy: 0.5137\n2/4 [==============>...............] - ETA: 2s - loss: 0.6497 - accuracy: 0.6367 \n3/4 [=====================>........] - ETA: 1s - loss: 0.6630 - accuracy: 0.6748\n4/4 [==============================] - ETA: 0s - loss: 0.6539 - accuracy: 0.6822\n4/4 [==============================] - 9s 1s/step - loss: 0.6539 - accuracy: 0.6822 - val_loss: 0.5991 - val_accuracy: 0.7440\nEpoch 2/10\n\n1/4 [======>.......................] - ETA: 3s - loss: 0.5901 - accuracy: 0.7520\n2/4 [==============>...............] - ETA: 2s - loss: 0.5802 - accuracy: 0.7534\n3/4 [=====================>........] - ETA: 1s - loss: 0.5842 - accuracy: 0.7432\n4/4 [==============================] - ETA: 0s - loss: 0.5820 - accuracy: 0.7445\n4/4 [==============================] - 4s 874ms/step - loss: 0.5820 - accuracy: 0.7445 - val_loss: 0.5701 - val_accuracy: 0.7440\nEpoch 3/10\n\n1/4 [======>.......................] - ETA: 3s - loss: 0.5642 - accuracy: 0.7510\n2/4 [==============>...............] - ETA: 2s - loss: 0.5648 - accuracy: 0.7500\n3/4 [=====================>........] - ETA: 1s - loss: 0.5691 - accuracy: 0.7461\n4/4 [==============================] - ETA: 0s - loss: 0.5709 - accuracy: 0.7445\n4/4 [==============================] - 4s 889ms/step - loss: 0.5709 - accuracy: 0.7445 - val_loss: 0.5678 - val_accuracy: 0.7440\nEpoch 4/10\n\n1/4 [======>.......................] - ETA: 3s - loss: 0.5463 - accuracy: 0.7666\n2/4 [==============>...............] - ETA: 2s - loss: 0.5630 - accuracy: 0.7505\n3/4 [=====================>........] - ETA: 1s - loss: 0.5638 - accuracy: 0.7497\n4/4 [==============================] - ETA: 0s - loss: 0.5688 - accuracy: 0.7445\n4/4 [==============================] - 4s 894ms/step - loss: 0.5688 - accuracy: 0.7445 - val_loss: 0.5681 - val_accuracy: 0.7440\nEpoch 5/10\n\n1/4 [======>.......................] - ETA: 3s - loss: 0.5665 - accuracy: 0.7451\n2/4 [==============>...............] - ETA: 2s - loss: 0.5648 - accuracy: 0.7476\n3/4 [=====================>........] - ETA: 1s - loss: 0.5683 - accuracy: 0.7432\n4/4 [==============================] - ETA: 0s - loss: 0.5666 - accuracy: 0.7445\n4/4 [==============================] - 4s 822ms/step - loss: 0.5666 - accuracy: 0.7445 - val_loss: 0.5620 - val_accuracy: 0.7440\nEpoch 6/10\n\n1/4 [======>.......................] - ETA: 3s - loss: 0.5480 - accuracy: 0.7578\n2/4 [==============>...............] - ETA: 2s - loss: 0.5569 - accuracy: 0.7485\n3/4 [=====================>........] - ETA: 1s - loss: 0.5583 - accuracy: 0.7464\n4/4 [==============================] - ETA: 0s - loss: 0.5605 - accuracy: 0.7445\n4/4 [==============================] - 4s 899ms/step - loss: 0.5605 - accuracy: 0.7445 - val_loss: 0.5540 - val_accuracy: 0.7440\nEpoch 7/10\n\n1/4 [======>.......................] - ETA: 3s - loss: 0.5624 - accuracy: 0.7373\n2/4 [==============>...............] - ETA: 2s - loss: 0.5412 - accuracy: 0.7544\n3/4 [=====================>........] - ETA: 1s - loss: 0.5495 - accuracy: 0.7441\n4/4 [==============================] - ETA: 0s - loss: 0.5481 - accuracy: 0.7445\n4/4 [==============================] - 4s 881ms/step - loss: 0.5481 - accuracy: 0.7445 - val_loss: 0.5328 - val_accuracy: 0.7440\nEpoch 8/10\n\n1/4 [======>.......................] - ETA: 3s - loss: 0.5141 - accuracy: 0.7598\n2/4 [==============>...............] - ETA: 2s - loss: 0.5132 - accuracy: 0.7549\n3/4 [=====================>........] - ETA: 1s - loss: 0.5186 - accuracy: 0.7458\n4/4 [==============================] - ETA: 0s - loss: 0.5186 - accuracy: 0.7445\n4/4 [==============================] - 4s 854ms/step - loss: 0.5186 - accuracy: 0.7445 - val_loss: 0.4862 - val_accuracy: 0.7440\nEpoch 9/10\n\n1/4 [======>.......................] - ETA: 3s - loss: 0.4837 - accuracy: 0.7363\n2/4 [==============>...............] - ETA: 2s - loss: 0.4642 - accuracy: 0.7466\n3/4 [=====================>........] - ETA: 1s - loss: 0.4490 - accuracy: 0.7461\n4/4 [==============================] - ETA: 0s - loss: 0.4451 - accuracy: 0.7445\n4/4 [==============================] - 4s 874ms/step - loss: 0.4451 - accuracy: 0.7445 - val_loss: 0.4084 - val_accuracy: 0.8024\nEpoch 10/10\n\n1/4 [======>.......................] - ETA: 3s - loss: 0.4311 - accuracy: 0.7910\n2/4 [==============>...............] - ETA: 2s - loss: 0.4309 - accuracy: 0.7891\n3/4 [=====================>........] - ETA: 1s - loss: 0.4266 - accuracy: 0.7975\n4/4 [==============================] - ETA: 0s - loss: 0.4191 - accuracy: 0.8044\n4/4 [==============================] - 4s 874ms/step - loss: 0.4191 - accuracy: 0.8044 - val_loss: 0.3784 - val_accuracy: 0.8714\n\n# history plot\nplot(history)\n\n\n\ntoc()\n\n42.69 sec elapsed\n\n\n\n\n\nDie Anwendung des Models auf die Datensätze train, validation und test ergibt nachstehende Metriken.\n\nMetrics train-DatensatzMetrics Validation-DatensatzMetrics test-Datensatz\n\n\n\n\nShow the Code\n# vorhersagen auf train-Datensatz\ndata_train_pred <- model_keras %>%\n  predict(data_train_x) %>%\n  k_argmax() %>%\n  as.vector()\n\n\n105/105 - 2s - 2s/epoch - 17ms/step\n\n\nShow the Code\n#Metriken train-Datensatz\nconf.matrix <-  confusionMatrix(\n  factor(data_train_pred, labels = c(\"no\", \"yes\")),\n  factor(data_train$class, labels = c(\"no\", \"yes\")),\n  positive = \"yes\"\n)\n\n#Anzeigen\nconf.matrix$byClass %>% round(digits = 3) * 100\n\n\n         Sensitivity          Specificity       Pos Pred Value \n                98.2                 68.1                 90.0 \n      Neg Pred Value            Precision               Recall \n                92.7                 90.0                 98.2 \n                  F1           Prevalence       Detection Rate \n                93.9                 74.4                 73.1 \nDetection Prevalence    Balanced Accuracy \n                81.2                 83.2 \n\n\n\n\n\n\nShow the Code\n# vorhersagen auf data_val (= Datensatz für Kreuzvalidierung)\ndata_val_pred <- model_keras %>%\n  predict(data_val_x) %>%\n  k_argmax() %>%\n  as.vector()\n\n\n27/27 - 0s - 351ms/epoch - 13ms/step\n\n\nShow the Code\n#Metriken für Teil von train-Datensatz (data_val)\nconf.matrix <-  confusionMatrix(\n  factor(data_val_pred, labels = c(\"no\", \"yes\")),\n  factor(data_val$class, labels = c(\"no\", \"yes\")),\n  positive = \"yes\"\n)\n\n#Anzeigen\nconf.matrix$byClass %>% round(digits = 3) * 100\n\n\n         Sensitivity          Specificity       Pos Pred Value \n                97.1                 58.1                 87.1 \n      Neg Pred Value            Precision               Recall \n                87.4                 87.1                 97.1 \n                  F1           Prevalence       Detection Rate \n                91.8                 74.4                 72.3 \nDetection Prevalence    Balanced Accuracy \n                83.0                 77.6 \n\n\n\n\n\n\nShow the Code\n# vorhersagen auf test-Datensatz\ndata_test_pred <- model_keras %>%\n  predict(data_test_x) %>%\n  k_argmax() %>%\n  as.vector()\n\n\n44/44 - 1s - 576ms/epoch - 13ms/step\n\n\nShow the Code\n#Metriken test-Datensatz\nconf.matrix <-  confusionMatrix(\n  factor(data_test_pred, labels = c(\"no\", \"yes\")),\n  factor(test_hate$class, labels = c(\"no\", \"yes\")),\n  positive = \"yes\"\n)\n\n#Anzeigen\nconf.matrix$byClass %>% round(digits = 3) * 100\n\n\n         Sensitivity          Specificity       Pos Pred Value \n                97.8                 58.7                 87.3 \n      Neg Pred Value            Precision               Recall \n                90.1                 87.3                 97.8 \n                  F1           Prevalence       Detection Rate \n                92.3                 74.4                 72.8 \nDetection Prevalence    Balanced Accuracy \n                83.3                 78.2"
  }
]